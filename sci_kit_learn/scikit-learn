一、机器学习：借助数学模型理解数据。学习过程即为调整参数让模型拟合得到的统计数据。
    分类：监督学习（supervised learning），无监督学习(unsupervised learning)，半监督学习(semi-supervised learning)
    监督学习（supervised learning）:对数据的若干特征和标签（类型）之间的关联性进行建模的过程。
        分类（classification）：标签（分类）是离散值，预测分类即定性分析。
        回归（regression）：标签是连续值，预测值即定量分析。
    无监督学习（unsupervised learning）:无标签建模，数据自我介绍的过程。
        聚类（clustering）:将数据分组。
        降维（dimensionality reduction）:让数据更简洁。
    半监督学习（semi-supervised learning）:数据标签不完整。
二、Scikit-learn
    1.数据表示（data Representation）:
        数据表--->特征矩阵（features matrix）--->目标（标签）数组（target array）
    2.API
    特点：
        统一性：所有对象共同连接一组方法和统一文档。
        内省：所有参数都是公共属性。
        限制对象层级：只有算法可以用类表示，数据集都是标准数据集表示（numpy数组、pandas dataframe、scipy稀疏矩阵）表示，参数名都是标准python字符串
        函数组合：很多任务都可以用一串基本算法实现。
        明智的默认值：当需要用户设置参数时sklearn预先定义了适当的默认值。
    常用步骤:
        （1）从Scikit-learn中导入适当的评估类，选择模型类。
        （2）用合适的数值对模型类进行实例化，配置模型超参数（hyperparameter）。
        （3）整理数据，通过前面介绍的方法获取特征矩阵和目标数组。
        （4）调用模型实例的fit()方法对数据进行拟合。
        （5）对新数据应用模型。
            监督学习：使用predict()方法预测新数据的标签。
            无监督学习：使用transform()或predict()方法转换或推断数据的性质。
三、超参选择和模型验证
    模型选择和超参选择是有效使用各种机器学习工具和技术的最重要阶段。我们需要一种方式来验证选中的模型和参数是否可以很好的拟合数据。
    1.模型验证（model validation）：选择模型和超参后，用训练数据进行学习，对比模型对一直数据的预测值和实际值的差异。
        错误方法：简单存储数据，然后把新数据于存储的一直数据进行对比来预测标签。
        正确方法：
            留出集（holdout set）:sklearn.model_selection.train_test_split()
                缺点：模型失去了一部分训练机会。
            交叉验证（cross validation）：将数据分成若干子集，依次选取每个子集作为验证集。
                1.n轮交叉验证
                from sklearn.model_selection import cross_val_score
                cross_val_score(model,X,y,cv=5)表示5轮交叉验证
                cross_val_predict:cross_val_score 很相像，不过不同于返回的是评测效果，cross_val_predict 返回的是estimator 的分类结果（或回归值）
                2.K折交叉验证:
                    这是将数据集分成K份的官方给定方案，所谓K折就是将数据集通过K次分割，使得所有数据既在训练集出现过，又在测试集出现过，当然，每次分割中不会有重叠。相当于无放回抽样。kf = KFold(n_splits=2)表示
                from sklearn.model_selection import KFold
                
                2.留P法（leavePout）:交叉验证的极端情况，每次留一个样本做验证。与lpo = LeavePOut(p=2)类似
                from sklearn.model_selection import LeaveOneOut
                from sklearn.model_selection import LeavePOut
                scores = cross_val_score(model,X,y,cv=LeaveOneOut(len(x)))
                scores.mean()：均值可以反映模型的准确性。
                3.ShuffleSplit
                    ShuffleSplit 咋一看用法跟LeavePOut 很像，其实两者完全不一样，LeavePOut 是使得数据集经过数次分割后，所有的测试集出现的元素的集合即是完整的数据集，即无放回的抽样，而ShuffleSplit 则是有放回的抽样，只能说经过一个足够大的抽样次数后，保证测试集出现了完成的数据集的倍数。
                4.StratifiedKFold
                    这个就比较好玩了，通过指定分组，对测试集进行无放回抽样。
                5.GroupKFold
                    这个跟StratifiedKFold 比较像，不过测试集是按照一定分组进行打乱的，即先分堆，然后把这些堆打乱，每个堆里的顺序还是固定不变的。
                6.eaveOneGroupOut
                    这个是在GroupKFold 上的基础上混乱度又减小了，按照给定的分组方式将测试集分割下来。
                7.LeavePGroupsOut
                    这个没啥可说的，跟上面那个一样，只是一个是单组，一个是多组
                8.GroupShuffleSplit
                    这个是有放回抽样
                9.TimeSeriesSplit
                    针对时间序列的处理，防止未来数据的使用，分割时是将数据进行从前到后切割（这个说法其实不太恰当，因为切割是延续性的。。）
                

    2.选择最优模型
        问题的答案往往与直觉相悖。
        改善模型能力的高低是决定机器学习实践者是否成功的标志
        1.偏差与方差的均衡：找出偏差（bias）与方差（variance）的均衡点。
            偏差（bias）:模型没有足够的灵活性来适应数据所有特征，就成为欠拟合，也称高偏差。
            方差（variance）:模型过于灵活，适应数据所有特征时也适应了随机误差，成为过拟合，也称高方差。
            R²（coefficient of determination）:判定系数
            R²=1 表示模型数据完全吻合
            R²=0 表示模型不比简单取均值更好。
            R²<0 表示模型性能很差
            对于高偏差模型，模型在验证集的表现与训练集的表现类似。
            对于高方差模型，模型在验证集的表现远远不如在训练集的表现。
            原理：
                total sum of squares  explained sum of squares   residual sum of squares
                R²=1-residual sum of squares/total sum of squares
                R²=1-（真实标签-预测标签）²/（真实标签-真实标签均值）²
                R²=1-偏差/方差
                偏差/方差  服从F分布
                正态总体平法和服从X方分布
            验证曲线：横轴：模型复杂度 纵轴：模型得分  训练的分和测试得分曲线
                特征：
                1.训练得分肯定高于验证得分。
                2.使用低复杂度（高偏差）的模型时，训练数据往往欠拟合，既模型对训练数据和测试数据都缺乏预测能力。
                3.使用高复杂度（高方差）模型时，训练数据往往过拟合，说明模型对训练数据预测能力强，对新数据预测能力差。
                4.使用复杂度适中的模型时，验证曲线得分最高。说明在该复杂度条件下偏差和方差达到均衡点。
            Scikit-learn验证曲线：
                多项式回归模型：
                    1次：y=ax+b
                    2次：y=ax**2+bx+c
                from sklearn.preprocessing import PolynomialFeatures 导入有多现实特征的处理器
                from sklearn.linear_model import LinearRegression 导入简单线性回归模型
                from sklearn.pipeline import make_pipeline 管道命令
                影响模型效果的两个因素：模型复杂度，训练数据集规模 
            学习曲线（learning curve）:训练得分，验证得分 学习曲线的特征： 
                1.特定复杂度的模型对较小的数据集容易过拟合，既此时训练的分高，验证得分低。 
                2.特定复杂度的模型对较大的数据集容易欠拟合，既随着数据集的增大，训练的分会不断降低，而验证的分会不断提高。 
                3.模型的验证集的得分永远不会高于训练集的得分，即两条曲线一直在靠近,但永远不会较差。 
                学习曲线的最重要特征： 随着训练样本的增加，分数会收敛到特定值。（如果你的数据多到使模型得分收敛，那么增加训练样本无济于事， 改善模型性能的唯一方法就是换模型，通常是换成更复杂的模型）。 
            验证实践: 三维网格中寻找最优值：多相似次数的搜索范围、回归模型是否拟合截距、模型是否需要标准化处理。 
            from sklearn.model_selection import GridSearchCV 
四、特征工程（feature engineering）: 特征工程：（将任意格式数据转换成具有良好特征的向量形式）找到与问题有关的任何信息，把它们转换成特征矩阵的数值。如：表示分类数据的特征、表示文本特征、表示图像特征、高模型复杂的的衍生特征、处理缺失数据。该过程也称向量化。 
    1.分类特征：非数值型数据类型是分类数据。 独热编码:增加额外的列，让0,1出现的对应的列分别表示每个分类值有或无。 
        from sklearn.feature_extraction import DictVectorizer 
        vec = DictVectorizer(sparse=False, dtype=int)
        sparse=False表示增加列，以0,1的出现与否表示每个分类有/无 
        sparse=True 表示用稀疏矩阵表示 vec.fit_transform(data) 
        分类特征工具： 
            sklearn.feature_extracton.DictVectorizer 
            sklearn.preprocessing.OneHotEncoder 
            sklearn.feature_extraction.FeatureHasher 
    2.文本特征: 单词统计是最常见的方法（统计每个单词出现的次数，放入表格中）。 
        from sklearn.feature_extraction.text import CountVectorizer 
        import pandas as pd vec=CountVectorizer() 
        X=vec.fit_transform(sample) 
        pd.DataFrame(X.toarray(),columns=vec.get_feature_names()) 
        这样就得到了没个单词出现次数的DataFrame 但是该方法的缺点是：原始单词统计会让一些常用词（如is）聚集太高权重，在分类算法中这样并不合理，解决该问题可以用TF-IDF（term frequency-inverse document frequency，词频逆文档频率，其大小与一个词的常见成都成反比）,通过单词在文档中出现的频率来衡量其权重。 
    3.图像特征：通过Scikit-Image模块来实现http://scikit-image.org 
    4.衍生特征：输入特征经过数学变换衍生出来的新特征。（基函数的回归（basis function regression）：将线性回归转换成多项式回归时，并不是通过改变模型，而是通过改变输入数据）。不改变模型，而是改变输出来改善模型效果--核方法（kernel method）
    5.缺失值填充：简单（列均值，中位数，众数），复杂（矩阵填充，模型处理）
        sklearn的Imputer可以实现一般的填充（均值、中位数、众数）
        from sklearn.preprocessing import Imputer
        imp=Imputer(strategy='mean')
        X2=imp.fit_transform(X)
    6.特征管道：
        如要实现以下三个操作：均值填充缺失，衍生特征转换为2次方，拟合线性回归模型。则可用管道操作。
        from sklearn.pipeline import make_pipeline
        model=make_pipeline(Imputer(strategy='mean'),PolynomialFeatures(degree=2),LinearRegression())
        合并三步为一步。
    
               
五、朴素贝叶斯分类（navie bayes classifier）：属于简单分类算法，通常适用于维度较高的数据集。
    1.特征：运行速度快，可调参数少。（快速、粗糙的解决方案）
        朴素：对每种标签的生成模型进行简单假设，就能找到每种类型生成模型的近似解，然后就可以使用贝叶斯分裂
        贝叶斯定理（Bayes's theorem）：描述统计量的条件概率公式，其成立的前提是：每种特征两两相互独立。
            P（L|特征）=P（特征|L）P（L）/P（特征）
            一种方法是标签对应后验概率的比值：P（L1|特征）/P（L2|特征）=P（特征|L1）P（L1）/P（特征|L2）P（L2）
            生成模型：计算每个标签的P（特征|Li）
            为每种标签设置生成模型是朴素贝叶斯分裂器训练的主要过程。
    2.高斯朴素贝叶斯（Gaussian naive Bayes）:假设每个标签的数据都服从简单的高斯分布。
        假设数据服从高斯分布且无协方差（no covariance，既相互独立），只要找出每个标签的所有样本点的均值和标准差再定义一个高斯分布，就可以拟合模型了。
        通常高斯朴素贝叶斯的分类边界是一条二次方曲线。
        缺点：由于高斯朴素贝叶斯最终分类结果只依赖于一开始的模型假设，经常看不到非常好的结果。（特征较多的时候，它扔是一种好方法）。
        贝叶斯主义（Bayesian formalism）：优质特征是它天生支持概率分布，我们可以用predict_proba方法计算样本属于摸个标签的概率。
        
        通过每种类型的生成模型，计算出任意数据点的似然估计P（特征|Li），然后根据贝叶斯定理计算出后延概率比值P（L1|特征）/P（L2|特征）=P（特征|L1）P（L1）/P（特征|L2）P（L2），而P（L1）和P（L2）很容易得到，从而确定每个数据点的最大可能标签。该步骤在sklearn.naive_bayes.GaussianNB评估器中实现
        
    3.多项式贝叶斯（multinomial naive Bayes）：假设特征服从简单多项式分布。
        多项式分布可以描述各种类型样本出现次数的概率，故多项式朴素贝叶斯非常适合用于描述出现次数或者次数比例特征。
        多项式分布：n次独立试验中（结果为发生或不发生），则发生k次的概率为C[n,k]p**k（1-p）**（n-k）
        通常用于文本分裂，其特征都是指待分类文本的单词出现次数（频次）
        可以用混淆矩阵来统计测试数据的真实标签与预测标签的结果。
        混淆矩阵：矩阵每一列代表预测值（predict class），每一行代表的是实际的类别（actual class）。这个名字来源于它可以非常容易的表明多个类别是否有混淆（也就是一个class被预测成另一个class）。如0,1交叉点表示多少实际的0被预测为1，1,1交叉点表示多少实际的1被预测为1
        查准率（Precision）和查全率（Recall） 我们将算法预测的结果分成四种情况： 
        1. 正确肯定（True Positive,TP）：预测为真，实际为真 
        2. 正确否定（True Negative,TN）：预测为假，实际为假 
        3. 错误肯定（False Positive,FP）：预测为真，实际为假 
        4. 错误否定（False Negative,FN）：预测为假，实际为真  
        则： 
        查准率=TP/（TP+FP）例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 
        查全率=TP/（TP+FN）例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。
      
      
    4.朴素贝叶斯应用场景：
        因为其对数据有严格的假设，因此它的训练效果通常比复杂的模型差，但其也有自己的优点：
            1.训练和预测的速度非常快。
            2.直接使用概率预测。
            3.容易理解。
            4.可调参数（如果有的话）较少。
        朴素贝叶斯分类器非常适合哦用于以下应用场景：
            1.假设分布函数与数据匹配（实际中很少见）。
            2.各种类型的区分度很高，模型复杂度不重要。
            3.非常高维度的数据模型复杂度不重要。
            最后两条看似不同其实彼此相关：随着数据集维度的增加，任何两点都不太可能逐渐靠近（毕竟他们得在每个维度上都足够接近才行），也就是说在新维度会增加样本数据信息量的假设条件下，高维度数据的簇中心点比低维度数据的簇中心点更加分散，因此，随着数据维度的不断增加，像朴素贝叶斯这样的简单分类器的分类效果会和复杂分类器一样，甚至更好。——只要有足够的数据，简单的模型也可以非常强大。
            
            
六、线性回归
    简单线性回归：一维数据
    基函数：对原始数据进行变换，将线性回归模型转化为非线性回归模型。
    y=a0+a1x+a2x**2+......该模型仍然是线性模型，也就是说稀疏an并不会彼此相乘或者相除。我们其实将一维的x投影到了高位空间，因此通过线性模型即可拟合出x和y的关系。
    高斯基函数：sklearn并没有内置高斯基函数
    虽然线性回归引入基函数会让模型变得更加灵活，但是也很容易造成过拟合。
    正则化：当基函数重叠的时候，通常就表明出现了过拟合：相邻基函数的系数相互抵消，这显然是有问题的，如果对较大的模型参数进行惩罚（penalize）,从而抑制模型剧烈波动，应该就可以解决这个问题了，这个惩罚机制被称为正则化（regularization）。
    岭回归（L2范数正则化）：正则化最常见的形式是岭回归（ridge regression）,又称L2范数正则化，又称吉洪诺夫正则化（Tikhonov regularization）。其处理方法是对模型系数平方和（L2范数）进行惩罚，模型拟合的惩罚项为：P=alpha*sum(系数平方）。alpha是一个自由参数用来控制惩罚力度。参数alpha时控制最终复杂度的关键，如果alpha>>0模型就恢复到标准线性回归结果，如果alpha>>正无穷，那么所有模型响应都会被压制。岭回归的一个重要优点是，它可以非常高效的计算——因此相比原始的线性回归模型，几乎没有消耗更多的计算机资源。
    Lasso正则化（L1范数）：其处理方法是对模型系数绝对值的和（L1范数）进行惩罚，P=alpha*sum(系数绝对值）。由于其几何特性，Lasso正则化倾向于构建稀疏模型，也就是说，它更喜欢将模型稀疏设为0。
    


七、支持向量机（support vector machine, SVM）
    SVM是非常强大的有监督学习算法，既可以用于分类也可以用于回归。
    生成分类法：先对每个类进行了随机分布的假设，然后用生成的模型估计新数据点的标签。
    判别分类法：不为每类数据建模，而是用一条分割线（二维空间中的直线或曲线）或者流行体（多维空间中的曲线、曲面等概念的推广）将各种类型分隔开。
        SVC（support vector classifier）：线性边界
        函数距离:γiˆ=yi(w∗xi+b)  已经远离的点可能通过调整中间线使其能够更加远离
        欧式距离：(Ax+By+C)/sqr(A**2+B**2)  不关心已经确定远离的点，更考虑靠近中间分割线的点
        优点之一：对远离数据点边界的数据点不敏感。
        将SVM模型与核函数组合使用，功能非常强大。将数据投影到高维空间，选择基函数比较困难：
            1.计算基函数在数据集上每个点的变换结果，让SVM算法从所有结果中筛选出最优解，这种基函数变换方式被称为核变换，是基于每对数据点之间的相似度（核函数）计算的。SVM模型自带程序来隐式计算核变换数据的拟合，既不须建立了完全的N维核函数的投影空间。在sklearn中我们可以应用核函数化的SVM模型将线性核转变为RBF（径向基函数）核，设置kernel模型超参即可。在机器学习中，核变换策略经常用于将快速线性方法变换成快速非线性方法，尤其是对于那些可以应用核函数技巧的模型。

    SVM优化：软化边界
        对于有交叉的数据，并没有清晰的分类边界，SVM提供了一些修正因子来软化边界，为了取得更好的拟合结果，它允许一些点位于边界线内。
        边界硬度C：如果C很大，边界就很硬，数据点不能再边界内生存，如果C小，边界线较软，有一些数据点可以穿越边界线。    
    
    总结：
        优点：
            1.模型依赖的支持向量较少，说明它模型精致，消耗内存少。
            2.一旦训练完成，预测速度快。
            3.由于模型只受边界线附近的点影响，因此他们对于高维数据的学习效果非常好。
            4.与核函数方法的配合极具通用性，能适应不同类型的数据。
        缺点：
            1.样本量N的增加，最差的训练时间复杂度会达到O(N**3)。经过高效处理后，也只能达到O(N**2)。大样本学习成本高。
            2.训练效果非常依赖于边界软化参数C的选择是否合理，这需要通过交叉检验自行搜索，当数据集大时，计算量大。
            3.预测结果不能直接进行概率解释，这一点可以通过内部交叉检验进行评估(VC的probability)但是评估过程计算量也非常大。
            
        由于这些条件的限制通常在其他简单、快速、调优难度小的方法不能满足需求时，才选择支持向量机，但是如果你对计算资源足以支撑SVM对数据集的训练和交叉检验，那么一定会获得很好的效果。
        


八、决策树与随机森林——无参数、集成方法 
    通过集成多个比较简单的评估模型形成积累效果，学习效果往往能超过多个简单评估模型形成的总和。既若干评估器的多数投票(ajority vote)的最终效果往往优于单个评估器投票的结果。
    1.随机森林是建立在决策树基础上的集成学习器，决策树直观的对事物进行分类和打标签，你只需问一系列问题就可以进行分类。
        二叉决策树：
            二叉树分支方法可以非常有效的进行分类一颗结构合理的决策树每个问题都可以将种类可能性减半，即使是对大量种类进行决策时，也可以很快缩小选择范围。问题：决策树分类边界是与特征轴平行的形式分割数据，既决策树每个节点都根据一个特征的法制将数据分成两组。
        决策树过拟合：
            决策树过拟合是决策树的一般属性——决策树非常容易陷得很深，因此往往会拟合局部数据，而没有对整个数据分布的大局观，这种过拟合就是模型训练的是数据的不同子集
            
            RandomForestRegressor可以拟合震荡曲线，适合处理多周期数据，不需要配置多周期模型。
            RandomForestClassifier用多棵决策树拟合曲线达到更好的效果
            

    2.评估器集成法：随机森林 BaggingClassifier
        装袋算法：通过组合多个过拟合评估器来降低过拟合程度的想法其实是一种集成学习方法。
        随机森林：装袋算法中使用平行评估器对数据进行有放回抽取集成（大杂烩），每个评估器都会对数据过拟合，通过求均值可以获得更好的分类结果。
    3.总结：
        随机森林优势：
            1.决策原理简单，故而它的训练和预测速度都非常快。多任务可以直接并行（多进程）计算（n_job参数），因为每棵树都是完全独立的。
            2.多棵树可以进行概率分类，多个评估器之间的多数投票可以给出概率的估计值（使用sklearn的predict_proba()方法）。
            3.无参数模型很灵活
            

九、主成分分析
    主成分分析（principal component analysis, PCA）:是一种基础降维算法，尤其适用于可视化、噪音过滤、特征抽取、和特征工程等领域。
    
        



            
    
    