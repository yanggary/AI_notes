一、机器学习：借助数学模型理解数据。学习过程即为调整参数让模型拟合得到的统计数据。
    分类：监督学习（supervised learning），无监督学习(unsupervised learning)，半监督学习(semi-supervised learning)
    监督学习（supervised learning）:对数据的若干特征和标签（类型）之间的关联性进行建模的过程。
        分类（classification）：标签（分类）是离散值，预测分类即定性分析。
        回归（regression）：标签是连续值，预测值即定量分析。
    无监督学习（unsupervised learning）:无标签建模，数据自我介绍的过程。
        聚类（clustering）:将数据分组。
        降维（dimensionality reduction）:让数据更简洁。
    半监督学习（semi-supervised learning）:数据标签不完整。
二、Scikit-learn
    1.数据表示（data Representation）:
        数据表--->特征矩阵（features matrix）--->目标（标签）数组（target array）
    2.API
    特点：
        统一性：所有对象共同连接一组方法和统一文档。
        内省：所有参数都是公共属性。
        限制对象层级：只有算法可以用类表示，数据集都是标准数据集表示（numpy数组、pandas dataframe、scipy稀疏矩阵）表示，参数名都是标准python字符串
        函数组合：很多任务都可以用一串基本算法实现。
        明智的默认值：当需要用户设置参数时sklearn预先定义了适当的默认值。
    常用步骤:
        （1）从Scikit-learn中导入适当的评估类，选择模型类。
        （2）用合适的数值对模型类进行实例化，配置模型超参数（hyperparameter）。
        （3）整理数据，通过前面介绍的方法获取特征矩阵和目标数组。
        （4）调用模型实例的fit()方法对数据进行拟合。
        （5）对新数据应用模型。
            监督学习：使用predict()方法预测新数据的标签。
            无监督学习：使用transform()或predict()方法转换或推断数据的性质。
三、超参选择和模型验证
    模型选择和超参选择是有效使用各种机器学习工具和技术的最重要阶段。我们需要一种方式来验证选中的模型和参数是否可以很好的拟合数据。
    1.模型验证（model validation）：选择模型和超参后，用训练数据进行学习，对比模型对一直数据的预测值和实际值的差异。
        错误方法：简单存储数据，然后把新数据于存储的一直数据进行对比来预测标签。
        正确方法：
            留出集（holdout set）:sklearn.model_selection.train_test_split()
                缺点：模型失去了一部分训练机会。
            交叉验证（cross validation）：将数据分成若干子集，依次选取每个子集作为验证集。
                1.n轮交叉验证
                from sklearn.model_selection import cross_val_score
                cross_val_score(model,X,y,cv=5)表示5轮交叉验证
                2.留一法（leave one out）:交叉验证的极端情况，每次留一个样本做验证。
                from sklearn.model_selection import LeaveOneOut
                scores = cross_val_score(model,X,y,cv=LeaveOneOut(len(x)))
                scores.mean()：均值可以反映模型的准确性。
    2.选择最优模型
        问题的答案往往与直觉相悖。
        改善模型能力的高低是决定机器学习实践者是否成功的标志
        1.偏差与方差的均衡：找出偏差（bias）与方差（variance）的均衡点。
            偏差（bias）:模型没有足够的灵活性来适应数据所有特征，就成为欠拟合，也称高偏差。
            方差（variance）:模型过于灵活，适应数据所有特征时也适应了随机误差，成为过拟合，也称高方差。
            R²（coefficient of determination）:判定系数
            R²=1 表示模型数据完全吻合
            R²=0 表示模型不比简单取均值更好。
            R²<0 表示模型性能很差
            对于高偏差模型，模型在验证集的表现与训练集的表现类似。
            对于高方差模型，模型在验证集的表现远远不如在训练集的表现。
            原理：
                total sum of squares  explained sum of squares   residual sum of squares
                R²=1-residual sum of squares/total sum of squares
                R²=1-（真实标签-预测标签）²/（真实标签-真实标签均值）²
                R²=1-偏差/方差
                偏差/方差  服从F分布
                正态总体平法和服从X方分布
            验证曲线：横轴：模型复杂度 纵轴：模型得分  训练的分和测试得分曲线
                特征：
                1.训练得分肯定高于验证得分。
                2.使用低复杂度（高偏差）的模型时，训练数据往往欠拟合，既模型对训练数据和测试数据都缺乏预测能力。
                3.使用高复杂度（高方差）模型时，训练数据往往过拟合，说明模型对训练数据预测能力强，对新数据预测能力差。
                4.使用复杂度适中的模型时，验证曲线得分最高。说明在该复杂度条件下偏差和方差达到均衡点。
            Scikit-learn验证曲线：
                多项式回归模型：
                    1次：y=ax+b
                    2次：y=ax**2+bx+c
                from sklearn.preprocessing import PolynomialFeatures 导入有多现实特征的处理器
                from sklearn.linear_model import LinearRegression 导入简单线性回归模型
                from sklearn.pipeline import make_pipeline 管道命令
                影响模型效果的两个因素：模型复杂度，训练数据集规模 
            学习曲线（learning curve）:训练得分，验证得分 学习曲线的特征： 
                1.特定复杂度的模型对较小的数据集容易过拟合，既此时训练的分高，验证得分低。 
                2.特定复杂度的模型对较大的数据集容易欠拟合，既随着数据集的增大，训练的分会不断降低，而验证的分会不断提高。 
                3.模型的验证集的得分永远不会高于训练集的得分，即两条曲线一直在靠近,但永远不会较差。 
                学习曲线的最重要特征： 随着训练样本的增加，分数会收敛到特定值。（如果你的数据多到使模型得分收敛，那么增加训练样本无济于事， 改善模型性能的唯一方法就是换模型，通常是换成更复杂的模型）。 
            验证实践: 三维网格中寻找最优值：多相似次数的搜索范围、回归模型是否拟合截距、模型是否需要标准化处理。 
            from sklearn.model_selection import GridSearchCV 
四、特征工程（feature engineering）: 特征工程：（将任意格式数据转换成具有良好特征的向量形式）找到与问题有关的任何信息，把它们转换成特征矩阵的数值。如：表示分类数据的特征、表示文本特征、表示图像特征、高模型复杂的的衍生特征、处理缺失数据。该过程也称向量化。 
    1.分类特征：非数值型数据类型是分类数据。 独热编码:增加额外的列，让0,1出现的对应的列分别表示每个分类值有或无。 
        from sklearn.feature_extraction import DictVectorizer 
        vec = DictVectorizer(sparse=False, dtype=int)
        sparse=False表示增加列，以0,1的出现与否表示每个分类有/无 
        sparse=True 表示用稀疏矩阵表示 vec.fit_transform(data) 
        分类特征工具： 
            sklearn.feature_extracton.DictVectorizer 
            sklearn.preprocessing.OneHotEncoder 
            sklearn.feature_extraction.FeatureHasher 
    2.文本特征: 单词统计是最常见的方法（统计每个单词出现的次数，放入表格中）。 
        from sklearn.feature_extraction.text import CountVectorizer 
        import pandas as pd vec=CountVectorizer() 
        X=vec.fit_transform(sample) 
        pd.DataFrame(X.toarray(),columns=vec.get_feature_names()) 
        这样就得到了没个单词出现次数的DataFrame 但是该方法的缺点是：原始单词统计会让一些常用词（如is）聚集太高权重，在分类算法中这样并不合理，解决该问题可以用TF-IDF（term frequency-inverse document frequency，词频逆文档频率，其大小与一个词的常见成都成反比）,通过单词在文档中出现的频率来衡量其权重。 
    3.图像特征：通过Scikit-Image模块来实现http://scikit-image.org 
    4.衍生特征：输入特征经过数学变换衍生出来的新特征。（基函数的回归（basis function regression）：将线性回归转换成多项式回归时，并不是通过改变模型，而是通过改变输入数据）。不改变模型，而是改变输出来改善模型效果--核方法（kernel method）
    5.缺失值填充：简单（列均值，中位数，众数），复杂（矩阵填充，模型处理）
        sklearn的Imputer可以实现一般的填充（均值、中位数、众数）
        from sklearn.preprocessing import Imputer
        imp=Imputer(strategy='mean')
        X2=imp.fit_transform(X)
    6.特征管道：
        如要实现以下三个操作：均值填充缺失，衍生特征转换为2次方，拟合线性回归模型。则可用管道操作。
        from sklearn.pipeline import make_pipeline
        model=make_pipeline(Imputer(strategy='mean'),PolynomialFeatures(degree=2),LinearRegression())
        合并三步为一步。
    7.sklearn.feature_extraction常用API介绍
        1.特征提取：sklearn.feature_extraction.DictVectorizer(dtype=<class 'numpy.float64'>,separator='=',sparse=True,sort=True)
            将<特征-值>映射转化为向量。字典类型的对象转化为numpy.array或者scipy.sparse特征值为string类型时，向量表示为one-hot二元编码，出现的string为1，其余为0.特征值为int等数字类型时，对应的值为相应的数字
            参数：
                dtype:特征值的类型。
                separator:可选，string。当特征值为string时，用来连接特征名称和值的符号，默认为'='。例，当特征名为'f',而特征值含有'pam'和'spam'时，one-hot对应的向量名为'f=pam'和'f=spam'
                sparse:boolean,可选。默认为True,转换过程中生成一个scipy.sparse矩阵。当数据多表示为one-hot类型时，占用内存过大，稀疏表示可以节约大量空间。
                sort:boolean,可选，默认为True。转化完成后对feature_names_和vocabulary_按字典序排列。
            属性：
                feature_names_:长度为n_features的列表，含有所有特征名称.
                vocabulary_:字典类型，特征名映射到特征在list中的index的字典对象
            方法：
                fit(X,y=None):学习一个将特征名称映射到索引的列表，返回值是其自身(DictVectorizer)。
                fit_transform(X,y=None):学习一个将特征名称映射到索引的列表，返回值是为对应的特征向量，一般2维等价于fit(X).transform(X)。
                get_feature_names():返回一个含有特征名称的列表，通过索引排序，如果含有one-hot表示的特征，则显示相应的特征名。
                get_params(deep=True):返回模型的参数（string到任何类型的映射）。
                inverse_transform(X,dict_type=<class 'dict'>):将转化好的特征向量恢复到转化之前的表示状态。X必须是通过transform或者fit_transform生成的向量。
                restrict(support,indices=False):对支持使用特征选择的模型进行特征限制，例如只选择前几个特征，support:矩阵类型，boolean或者索引列表，一般是feature selectors.get_support()的返回值。indices:boolean，可选，表示support是不是索引的列表返回值是其自身(DictVectorizer)
                set_params(**params):设置DictVectorizer的参数。
                transform(X):学习一个将特征名称映射到索引的列表，返回值是为对应的特征向量，一般2维。
        2.sklearn.feature_extraction.FeatureHasher(n_features=1048576,input_type='dict',dytpe<float64>,alternate_sign=True,non_negative=False)：采用哈希方法将象征性的特征序列转化为scipy.sparse矩阵，可以节约时间和空间。
        3.sklearn.feature_extraction.text.CountVectorizer(input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\b\w\w+\b',ngram_range=(1,1),analyzer='word',max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=<'numpy.int64'>):
        CountVectorizer:返回的出现次数的稀疏矩阵，TfidfTransformer：返回频率逆序文档频率归一化的发生次数矩阵（term frequency-inverse document frequency，词频逆文档频率，其大小与一个词的常见程度成反比）。参数属性方法基本相同。
        参数：
            input：string {‘filename’, ‘file’, ‘content’}：if filename，输入的是要解析的文件序列，if file,可读文件，if content,字符串或字节项。
            encoding : string, ‘utf-8’ by default.
            decode_error : {‘strict’, ‘ignore’, ‘replace’}：遇到不能识别的字符时的处理方式，默认的是strict，意思是如果解码错误就汇报。其他的可选值为ignore’ 和‘replace’。
            strip_accents : {‘ascii’, ‘unicode’, None}：去掉口音（这里我觉得应该是‘特殊字符’），ascii是一个对字符进行快速映射的方法。unicode是一种较慢的方法，但是可以对任何字符使用。
            analyzer : string, {‘word’, ‘char’, ‘char_wb’} or callable：特征是否应该转化为单词或者n-grams（一种简化单词的方式，可以将注入book和books综合成一个单次），选项‘char_wb’仅将文档中的单词表示成为n-grams的形式），如果一个用来提取特征的可调用函数，被传递进来，那么就按照被传递的函数进行处理）。
            preprocessor : callable or None (default)：改写预测处理的函数，同时保留标记和n-grams的处理过程。
            tokenizer : callable or None (default)：改写字符串标记步骤，同时保留预处理和n-grams的步骤。
            ngram_range : tuple (min_n, max_n)：对于不同的n-grams的提取，n-value变化的上界和下界，所有的min_n <= n <= max_n 的n值都会被使用
            stop_words : string {‘english’}, list, or None (default)：如果是字符串，那么它将被传递给_check_stop_list，并且返回停用列表，现在只支持英文的停用词列表，如果是列表，那么这个列表将被认为包含了停用词，这些词将被从文档中的单词删除掉，如果是none，那么就不使用停用词，max_df可以被设置成在[0.7，1.0]之间的值，来自动的检测和过滤掉基于其语料库的停用词。
            lowercase : boolean, default True：全部改成小写。
            token_pattern : string：构成token的正则表达式，它仅仅在tokenize == ‘word’的时候才使用。默认的正则表达式是选择两个或者两个以上的字符（标点符号是被忽略的，仅仅当作token的分割符）
            max_df : float in range [0.0, 1.0] or int, optional, 1.0 by default：当构建字典的时候，忽略词频明显高于给的那个的阈值（语料库的停用词），如果是浮点数，那么参数代表了一个文档的比例，当是整数的时候，代表的是计数值。当字典不是none的时候，这个参数会被忽略。
            min_df : float in range [0.0, 1.0] or int, optional, 1 by default：当构建忽略词的字典的时候，这些忽略词要求要严格的低于给定的阈值。这个值在书中称之为截至值，如果是浮点数，那么代表的是文档中的比例，如果是整数那么就是绝对计数值。如果字典不是空的话，这个参数被忽略。
            max_features : optional, None by default：如果不是none，构建词典的时候仅仅考虑语料库里词频最高的那些特征，如果词典非空，那么这个参数被忽略。
            vocabulary : Mapping or iterable, optional：是以词为键，并且值可以在特征矩阵里可以索引的，或者是一个单词的迭代（译注：这里不是很清楚是什么意思），如果词典不是给定的，那么将会从输入的文档库中选择。
            binary : boolean, False by default.：如果是真，那么所有非零的计数将被设置成1。这对于离散的只针对二值事件而非整数计数的概率模型很有用。
            dtype : type, optional：通过fit_transform或者transform处理后返回的矩阵类型。
        方法：
            build_analyzer（）返回处理预处理和标记化的可调用
            build_preprocessor（）返回一个函数，以便在标记化之前对文本进行预处理
            build_tokenizer（）返回一个将字符串分成令牌序列的函数
            decode（DOC）将输入解码为一串Unicode码元
            fit（raw_documents [，y]）从训练集学习词汇和idf。
            fit_transform（raw_documents [，y]）学习词汇和idf，返回术语文档矩阵。
            get_feature_names（）从特征整数索引到特征名称的数组映射
            get_params（[deep]）获取此估计器的参数。
            get_stop_words（）构建或获取有效的停止词列表
            inverse_transform（X）每个文档在X中返回非零条目的条款。
            set_params（** PARAMS）设置该估计器的参数。
            transform（raw_documents [，copy]）将文档转换为文档术语矩阵。
                 
        4.sklearn.feature_extraction.image.PatchExtractor（patch_size=None,max_patches=None,random_state=None）
            transform(X):将图片样本X转化为小块数据矩阵。X : array, shape = (n_samples, image_height, image_width) or(n_samples, image_height, image_width, n_channels)
            返回值：patches : array, shape = (n_patches, patch_height, patch_width) or(n_patches, patch_height, patch_width, n_channels)
               
五、朴素贝叶斯分类（navie bayes classifier）：属于简单分类算法，通常适用于维度较高的数据集。
    1.特征：运行速度快，可调参数少。（快速、粗糙的解决方案）
        朴素：对每种标签的生成模型进行简单假设，就能找到每种类型生成模型的近似解，然后就可以使用贝叶斯分裂
        贝叶斯定理（Bayes's theorem）：描述统计量的条件概率公式，其成立的前提是：每种特征两两相互独立。
            P（L|特征）=P（特征|L）P（L）/P（特征）
            一种方法是标签对应后验概率的比值：P（L1|特征）/P（L2|特征）=P（特征|L1）P（L1）/P（特征|L2）P（L2）
            生成模型：计算每个标签的P（特征|Li）
            为每种标签设置生成模型是朴素贝叶斯分裂器训练的主要过程。
    2.高斯朴素贝叶斯（Gaussian naive Bayes）:假设每个标签的数据都服从简单的高斯分布。
        假设数据服从高斯分布且无协方差（no covariance，既相互独立），只要找出每个标签的所有样本点的均值和标准差再定义一个高斯分布，就可以拟合模型了。
        通常高斯朴素贝叶斯的分类边界是一条二次方曲线。
        缺点：由于高斯朴素贝叶斯最终分类结果只依赖于一开始的模型假设，经常看不到非常好的结果。（特征较多的时候，它扔是一种好方法）。
        贝叶斯主义（Bayesian formalism）：优质特征是它天生支持概率分布，我们可以用predict_proba方法计算样本属于摸个标签的概率。
        sklearn.datasets.make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=1.0, center_box=(-10.0, 10.0), shuffle=True, random_state=None)
        返回值（returns）:
            X : array of shape [n_samples, n_features] :The generated samples. 生成的样本数据集。 
            y : array of shape [n_samples] : The integer labels for cluster membership of each sample. 样本数据集的标签。
        parameters:
            n_samples: int, optional (default=100) :The total number of points equally divided among clusters. 
    待生成的样本的总数。
            n_features: int, optional (default=2) :The number of features for each sample.每个样本的特征数。 
            centers: int or array of shape [n_centers, n_features], optional (default=3) :The number of centers to generate, or the fixed center locations. 要生成的样本中心（类别）数，或者是确定的中心点。 
            cluster_std: float or sequence of floats, optional (default=1.0) :The standard deviation of the clusters.每个类别的方差，例如我们希望生成2类数据，其中一类比另一类具有更大的方差，可以将cluster_std设置为[1.0,3.0]。 
            center_box: pair of floats (min, max), optional (default=(-10.0, 10.0)) :The bounding box for each cluster center when centers are generated at random. 
            shuffle: boolean, optional (default=True) :Shuffle the samples. 打乱样本。
            random_state: int, RandomState instance or None, optional (default=None) :相当于random中的seed
        通过每种类型的生成模型，计算出任意数据点的似然估计P（特征|Li），然后根据贝叶斯定理计算出后延概率比值P（L1|特征）/P（L2|特征）=P（特征|L1）P（L1）/P（特征|L2）P（L2），而P（L1）和P（L2）很容易得到，从而确定每个数据点的最大可能标签。该步骤在sklearn.naive_bayes.GaussianNB评估器中实现
        from sklearn.navie_bayes import GaussianNB
        其模型对象的属性（attributions）:
            priors属性：获取各个类标签对应的先验概率
            class_prior_属性：同priors一样，都是获取各个类标记对应的先验概率，区别在于priors属性返回列表，class_prior_返回的是数组
            class_count_属性：获取各类标记对应的训练样本数
            theta_属性：获取各个类标记在各个特征上的均值
            sigma_属性：获取各个类标记在各个特征上的方差
        方法（method）:
            get_params(deep=True)：返回priors与其参数值组成字典
            set_params(**params)：设置估计器priors参数
            fit(X, y, sample_weight=None)：训练样本，X表示特征向量，y类标记，sample_weight表各样本权重数组
            partial_fit(X, y, classes=None, sample_weight=None)：增量式训练，当训练数据集数据量非常大，不能一次性全部载入内存时，可以将数据集划分若干份，重复调用partial_fit在线学习模型参数，在第一次调用partial_fit函数时，必须制定classes参数[1,2]表示有1和2两个分类，在随后的调用可以忽略。
            predict(X)：直接输出测试集预测的类标记
            predict_proba(X)：输出测试样本在各个类标记预测概率值,predict_proba(X).round(2)表示结果保留2位小数。
            predict_log_proba(X)：输出测试样本在各个类标记上预测概率值对应对数值
            score(X, y, sample_weight=None)：返回测试样本映射到指定类标记上的得分(准确率)
    3.多项式贝叶斯（multinomial naive Bayes）：假设特征服从简单多项式分布。
        多项式分布可以描述各种类型样本出现次数的概率，故多项式朴素贝叶斯非常适合用于描述出现次数或者次数比例特征。
        多项式分布：n次独立试验中（结果为发生或不发生），则发生k次的概率为C[n,k]p**k（1-p）**（n-k）
        通常用于文本分裂，其特征都是指待分类文本的单词出现次数（频次）
        可以用混淆矩阵来统计测试数据的真实标签与预测标签的结果。
        混淆矩阵：矩阵每一列代表预测值（predict class），每一行代表的是实际的类别（actual class）。这个名字来源于它可以非常容易的表明多个类别是否有混淆（也就是一个class被预测成另一个class）。如0,1交叉点表示多少实际的0被预测为1，1,1交叉点表示多少实际的1被预测为1
        查准率（Precision）和查全率（Recall） 我们将算法预测的结果分成四种情况： 
        1. 正确肯定（True Positive,TP）：预测为真，实际为真 
        2. 正确否定（True Negative,TN）：预测为假，实际为假 
        3. 错误肯定（False Positive,FP）：预测为真，实际为假 
        4. 错误否定（False Negative,FN）：预测为假，实际为真  
        则： 
        查准率=TP/（TP+FP）例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 
        查全率=TP/（TP+FN）例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。
        总结1.sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None) 
        parameters:
            alpha：浮点型，可选项，默认1.0，添加拉普拉修/Lidstone平滑参数
            fit_prior：布尔型，可选项，默认True，表示是否学习先验概率，参数为False表示所有类标记具有相同的先验概率
            class_prior：类似数组，数组大小为(n_classes,)，默认None，类先验概率
        训练后的attributions:
            class_log_prior_：各类标记的平滑先验概率对数值，其取值会受fit_prior和class_prior参数的影响
                a、若指定了class_prior参数，不管fit_prior为True或False，class_log_prior_取值是class_prior转换成log后的结果
                b、若fit_prior参数为False，class_prior=None，则各类标记的先验概率相同等于类标记总个数N分之一
                c、若fit_prior参数为True，class_prior=None，则各类标记的先验概率相同等于各类标记个数除以各类标记个数之和
            intercept_：将多项式朴素贝叶斯解释的class_log_prior_映射为线性模型，其值和class_log_propr相同
            feature_log_prob_：指定类的各特征概率(条件概率)对数值，返回形状为(n_classes, n_features)数组
            特征的条件概率=（指定类下指定特征出现的次数+alpha）/（指定类下所有特征出现次数之和+类的可能取值个数*alpha）
            coef_：将多项式朴素贝叶斯解释feature_log_prob_映射成线性模型，其值和feature_log_prob相同
            class_count_：训练样本中各类别对应的样本数，按类的顺序排序输出
            feature_count_：各类别各个特征出现的次数，返回形状为(n_classes, n_features)数组
        方法（method）:
            fit(X, y, sample_weight=None)：根据X、y训练模型
            get_params(deep=True)：获取分类器的参数，以各参数字典形式返回
            partial_fit(X, y, classes=None, sample_weight=None)：对于数据量大时，提供增量式训练，在线学习模型参数，参数X可以是类似数组或稀疏矩阵，在第一次调用函数，必须制定classes参数，随后调用时可以忽略。
            predict(X)：在测试集X上预测，输出X对应目标值
            predict_log_proba(X)：测试样本划分到各个类的概率对数值
            predict_proba(X)：输出测试样本划分到各个类别的概率值
            score(X, y, sample_weight=None)：输出对测试样本的预测准确率的平均值
            set_params(**params)：设置估计器的参数
         总结2.sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)
            y_true: 是样本真实分类结果，
            y_pred: 是样本预测分类结果 
            labels：是所给出的类别，通过这个可对类别进行选择 
            sample_weight : 样本权重
    4.朴素贝叶斯应用场景：
        因为其对数据有严格的假设，因此它的训练效果通常比复杂的模型差，但其也有自己的优点：
            1.训练和预测的速度非常快。
            2.直接使用概率预测。
            3.容易理解。
            4.可调参数（如果有的话）较少。
        朴素贝叶斯分类器非常适合哦用于以下应用场景：
            1.假设分布函数与数据匹配（实际中很少见）。
            2.各种类型的区分度很高，模型复杂度不重要。
            3.非常高维度的数据模型复杂度不重要。
            最后两条看似不同其实彼此相关：随着数据集维度的增加，任何两点都不太可能逐渐靠近（毕竟他们得在每个维度上都足够接近才行），也就是说在新维度会增加样本数据信息量的假设条件下，高维度数据的簇中心点比低维度数据的簇中心点更加分散，因此，随着数据维度的不断增加，像朴素贝叶斯这样的简单分类器的分类效果会和复杂分类器一样，甚至更好。——只要有足够的数据，简单的模型也可以非常强大。
            
            
六、线性回归
    简单线性回归：一维数据
    基函数：对原始数据进行变换，将线性回归模型转化为非线性回归模型。
    y=a0+a1x+a2x**2+......该模型仍然是线性模型，也就是说稀疏an并不会彼此相乘或者相除。我们其实将一维的x投影到了高位空间，因此通过线性模型即可拟合出x和y的关系。
    高斯基函数：sklearn并没有内置高斯基函数
    知识点1.
        sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False,copy_X=True, n_jobs=1)
            parameters:
                fit_intercept：布尔型，默认为True，若参数值为True时，代表训练模型需要加一个截距项；若参数为False时，代表模型无需加截距项。
                normalize：布尔型，默认为False，若fit_intercept参数设置False时，normalize参数无需设置；若normalize设置为True时，则输入的样本数据(X-X均值)/||X||；若设置normalize=False时，在训练模型前， 可以使用sklearn.preprocessing.StandardScaler进行标准化处理。
            attributions:
                coef_：回归系数(斜率)
                intercept_：截距项
            method:
                fit(X, y, sample_weight=None)
                predict(X)
                score(X, y, sample_weight=None)，其结果等于1-(((y_true - y_pred) **2).sum() / ((y_true - y_true.mean()) ** 2).sum())
    知识点2.
        sklearn.preprocessing.PolynomialFeatures()
            degree：控制多项式的维度
            interaction_only： 默认为False，如果指定为True，那么就不会有特征自己和自己结合的项，二次项中没有x^2和y^2（1,x,y,x^2,xy, y^2）。
            include_bias：默认为True。如果为True的话，那么就会有上面的1那一项。
            
            



            
    
    