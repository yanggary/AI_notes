一、机器学习：借助数学模型理解数据。学习过程即为调整参数让模型拟合得到的统计数据。
    分类：监督学习（supervised learning），无监督学习(unsupervised learning)，半监督学习(semi-supervised learning)
    监督学习（supervised learning）:对数据的若干特征和标签（类型）之间的关联性进行建模的过程。
        分类（classification）：标签（分类）是离散值，预测分类即定性分析。
        回归（regression）：标签是连续值，预测值即定量分析。
    无监督学习（unsupervised learning）:无标签建模，数据自我介绍的过程。
        聚类（clustering）:将数据分组。
        降维（dimensionality reduction）:让数据更简洁。
    半监督学习（semi-supervised learning）:数据标签不完整。
二、Scikit-learn
    1.数据表示（data Representation）:
        数据表--->特征矩阵（features matrix）--->目标（标签）数组（target array）
    2.API
    特点：
        统一性：所有对象共同连接一组方法和统一文档。
        内省：所有参数都是公共属性。
        限制对象层级：只有算法可以用类表示，数据集都是标准数据集表示（numpy数组、pandas dataframe、scipy稀疏矩阵）表示，参数名都是标准python字符串
        函数组合：很多任务都可以用一串基本算法实现。
        明智的默认值：当需要用户设置参数时sklearn预先定义了适当的默认值。
    常用步骤:
        （1）从Scikit-learn中导入适当的评估类，选择模型类。
        （2）用合适的数值对模型类进行实例化，配置模型超参数（hyperparameter）。
        （3）整理数据，通过前面介绍的方法获取特征矩阵和目标数组。
        （4）调用模型实例的fit()方法对数据进行拟合。
        （5）对新数据应用模型。
            监督学习：使用predict()方法预测新数据的标签。
            无监督学习：使用transform()或predict()方法转换或推断数据的性质。
三、超参选择和模型验证
    模型选择和超参选择是有效使用各种机器学习工具和技术的最重要阶段。我们需要一种方式来验证选中的模型和参数是否可以很好的拟合数据。
    1.模型验证（model validation）：选择模型和超参后，用训练数据进行学习，对比模型对一直数据的预测值和实际值的差异。
        错误方法：简单存储数据，然后把新数据于存储的一直数据进行对比来预测标签。
        正确方法：
            留出集（holdout set）:sklearn.model_selection.train_test_split()
                缺点：模型失去了一部分训练机会。
            交叉验证（cross validation）：将数据分成若干子集，依次选取每个子集作为验证集。
                1.n轮交叉验证
                from sklearn.model_selection import cross_val_score
                cross_val_score(model,X,y,cv=5)表示5轮交叉验证
                2.留一法（leave one out）:交叉验证的极端情况，每次留一个样本做验证。
                from sklearn.model_selection import LeaveOneOut
                scores = cross_val_score(model,X,y,cv=LeaveOneOut(len(x)))
                scores.mean()：均值可以反映模型的准确性。
    2.选择最优模型
        问题的答案往往与直觉相悖。
        改善模型能力的高低是决定机器学习实践者是否成功的标志
        1.偏差与方差的均衡：找出偏差（bias）与方差（variance）的均衡点。
            偏差（bias）:模型没有足够的灵活性来适应数据所有特征，就成为欠拟合，也称高偏差。
            方差（variance）:模型过于灵活，适应数据所有特征时也适应了随机误差，成为过拟合，也称高方差。
            R²（coefficient of determination）:判定系数
            R²=1 表示模型数据完全吻合
            R²=0 表示模型不比简单取均值更好。
            R²<0 表示模型性能很差
            对于高偏差模型，模型在验证集的表现与训练集的表现类似。
            对于高方差模型，模型在验证集的表现远远不如在训练集的表现。
            原理：
                total sum of squares  explained sum of squares   residual sum of squares
                R²=1-residual sum of squares/total sum of squares
                R²=1-（真实标签-预测标签）²/（真实标签-真实标签均值）²
                R²=1-偏差/方差
                偏差/方差  服从F分布
                正态总体平法和服从X方分布
            验证曲线：横轴：模型复杂度 纵轴：模型得分  训练的分和测试得分曲线
                特征：
                1.训练得分肯定高于验证得分。
                2.使用低复杂度（高偏差）的模型时，训练数据往往欠拟合，既模型对训练数据和测试数据都缺乏预测能力。
                3.使用高复杂度（高方差）模型时，训练数据往往过拟合，说明模型对训练数据预测能力强，对新数据预测能力差。
                4.使用复杂度适中的模型时，验证曲线得分最高。说明在该复杂度条件下偏差和方差达到均衡点。
            Scikit-learn验证曲线：
                多项式回归模型：
                    1次：y=ax+b
                    2次：y=ax**2+bx+c
                from sklearn.preprocessing import PolynomialFeatures 导入有多现实特征的处理器
                from sklearn.linear_model import LinearRegression 导入简单线性回归模型
                from sklearn.pipeline import make_pipeline 管道命令
                影响模型效果的两个因素：模型复杂度，训练数据集规模 
            学习曲线（learning curve）:训练得分，验证得分 学习曲线的特征： 
                1.特定复杂度的模型对较小的数据集容易过拟合，既此时训练的分高，验证得分低。 
                2.特定复杂度的模型对较大的数据集容易欠拟合，既随着数据集的增大，训练的分会不断降低，而验证的分会不断提高。 
                3.模型的验证集的得分永远不会高于训练集的得分，即两条曲线一直在靠近,但永远不会较差。 
                学习曲线的最重要特征： 随着训练样本的增加，分数会收敛到特定值。（如果你的数据多到使模型得分收敛，那么增加训练样本无济于事， 改善模型性能的唯一方法就是换模型，通常是换成更复杂的模型）。 
            验证实践: 三维网格中寻找最优值：多相似次数的搜索范围、回归模型是否拟合截距、模型是否需要标准化处理。 
            from sklearn.model_selection import GridSearchCV 
四、特征工程（feature engineering）: 特征工程：（将任意格式数据转换成具有良好特征的向量形式）找到与问题有关的任何信息，把它们转换成特征矩阵的数值。如：表示分类数据的特征、表示文本特征、表示图像特征、高模型复杂的的衍生特征、处理缺失数据。该过程也称向量化。 
    1.分类特征：非数值型数据类型是分类数据。 独热编码:增加额外的列，让0,1出现的对应的列分别表示每个分类值有或无。 
        from sklearn.feature_extraction import DictVectorizer 
        vec = DictVectorizer(sparse=False, dtype=int)
        sparse=False表示增加列，以0,1的出现与否表示每个分类有/无 
        sparse=True 表示用稀疏矩阵表示 vec.fit_transform(data) 
        分类特征工具： 
            sklearn.feature_extracton.DictVectorizer 
            sklearn.preprocessing.OneHotEncoder 
            sklearn.feature_extraction.FeatureHasher 
    2.文本特征: 单词统计是最常见的方法（统计每个单词出现的次数，放入表格中）。 
        from sklearn.feature_extraction.text import CountVectorizer 
        import pandas as pd vec=CountVectorizer() 
        X=vec.fit_transform(sample) 
        pd.DataFrame(X.toarray(),columns=vec.get_feature_names()) 
        这样就得到了没个单词出现次数的DataFrame 但是该方法的缺点是：原始单词统计会让一些常用词（如is）聚集太高权重，在分类算法中这样并不合理，解决该问题可以用TF-IDF（term frequency-inverse document frequency，词频逆文档频率，其大小与一个词的常见成都成反比）,通过单词在文档中出现的频率来衡量其权重。 
    3.图像特征：通过Scikit-Image模块来实现http://scikit-image.org 
    4.衍生特征：输入特征经过数学变换衍生出来的新特征。（基函数的回归（basis function regression）：将线性回归转换成多项式回归时，并不是通过改变模型，而是通过改变输入数据）。不改变模型，而是改变输出来改善模型效果--核方法（kernel method）
    5.缺失值填充：简单（列均值，中位数，众数），复杂（矩阵填充，模型处理）
        sklearn的Imputer可以实现一般的填充（均值、中位数、众数）
        from sklearn.preprocessing import Imputer
        imp=Imputer(strategy='mean')
        X2=imp.fit_transform(X)
    6.特征管道：
        如要实现以下三个操作：均值填充缺失，衍生特征转换为2次方，拟合线性回归模型。则可用管道操作。
        from sklearn.pipeline import make_pipeline
        model=make_pipeline(Imputer(strategy='mean'),PolynomialFeatures(degree=2),LinearRegression())
        合并三步为一步。
五、朴素贝叶斯分类（navie bayes classifier）：属于简单分类算法，通常适用于维度较高的数据集。
    1.特征：运行速度快，可调参数少。（快速、粗糙的解决方案）
        朴素：对每种标签的生成模型进行简单假设，就能找到每种类型生成模型的近似解，然后就可以使用贝叶斯分裂
        贝叶斯定理（Bayes's theorem）：描述统计量的条件概率公式，其成立的前提是：每种特征两两相互独立。
            P（L|特征）=P（特征|L）P（L）/P（特征）
            一种方法是标签对应后验概率的比值：P（L1|特征）/P（L2|特征）=P（特征|L1）P（L1）/P（特征|L2）P（L2）
            生成模型：计算每个标签的P（特征|Li）
            为每种标签设置生成模型是朴素贝叶斯分裂器训练的主要过程。
    2.高斯朴素贝叶斯（Gaussian naive Bayes）:假设每个标签的数据都服从简单的高斯分布。
        假设数据服从高斯分布且无协方差（no covariance，既相互独立），只要找出每个标签的所有样本点的均值和标准差再定义一个高斯分布，就可以拟合模型了。
        sklearn.datasets.make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=1.0, center_box=(-10.0, 10.0), shuffle=True, random_state=None)
        returns:
            X : array of shape [n_samples, n_features] :The generated samples. 生成的样本数据集。 
            y : array of shape [n_samples] : The integer labels for cluster membership of each sample. 样本数据集的标签。
        parameters:
            n_samples: int, optional (default=100) :The total number of points equally divided among clusters. 
    待生成的样本的总数。
            n_features: int, optional (default=2) :The number of features for each sample.每个样本的特征数。 
            centers: int or array of shape [n_centers, n_features], optional (default=3) :The number of centers to generate, or the fixed center locations. 要生成的样本中心（类别）数，或者是确定的中心点。 
            cluster_std: float or sequence of floats, optional (default=1.0) :The standard deviation of the clusters.每个类别的方差，例如我们希望生成2类数据，其中一类比另一类具有更大的方差，可以将cluster_std设置为[1.0,3.0]。 
            center_box: pair of floats (min, max), optional (default=(-10.0, 10.0)) :The bounding box for each cluster center when centers are generated at random. 
            shuffle: boolean, optional (default=True) :Shuffle the samples. 打乱样本。
            random_state: int, RandomState instance or None, optional (default=None) :相当于random中的seed
        通过每种类型的生成模型，计算出任意数据点的似然估计P（特征|Li），然后根据贝叶斯定理计算出后延概率比值P（L1|特征）/P（L2|特征）=P（特征|L1）P（L1）/P（特征|L2）P（L2），而P（L1）和P（L2）很容易得到，从而确定每个数据点的最大可能标签。该步骤在sklearn.naive_bayes.GaussianNB评估器中实现
        from sklearn.navie_bayes import GaussianNB
        其模型对象的属性（attributions）:
        priors属性：获取各个类标签对应的先验概率
        class_prior_属性：同priors一样，都是获取各个类标记对应的先验概率，区别在于priors属性返回列表，class_prior_返回的是数组
        class_count_属性：获取各类标记对应的训练样本数
        theta_属性：获取各个类标记在各个特征上的均值
        sigma_属性：获取各个类标记在各个特征上的方差
        get_params(deep=True)：返回priors与其参数值组成字典
        set_params(**params)：设置估计器priors参数
        fit(X, y, sample_weight=None)：训练样本，X表示特征向量，y类标记，sample_weight表各样本权重数组
        partial_fit(X, y, classes=None, sample_weight=None)：增量式训练，当训练数据集数据量非常大，不能一次性全部载入内存时，可以将数据集划分若干份，重复调用partial_fit在线学习模型参数，在第一次调用partial_fit函数时，必须制定classes参数[1,2]表示有1和2两个分类，在随后的调用可以忽略。
        predict(X)：直接输出测试集预测的类标记
        predict_proba(X)：输出测试样本在各个类标记预测概率值
        predict_log_proba(X)：输出测试样本在各个类标记上预测概率值对应对数值
        score(X, y, sample_weight=None)：返回测试样本映射到指定类标记上的得分(准确率)
        多项式贝叶斯sklearn.naive_bayes.MultinomialNB后续可学习！！！！
        



            
    
    