一、机器学习：借助数学模型理解数据。学习过程即为调整参数让模型拟合得到的统计数据。
    分类：监督学习（supervised learning），无监督学习(unsupervised learning)，半监督学习(semi-supervised learning)
    监督学习（supervised learning）:对数据的若干特征和标签（类型）之间的关联性进行建模的过程。
        分类（classification）：标签（分类）是离散值，预测分类即定性分析。
        回归（regression）：标签是连续值，预测值即定量分析。
    无监督学习（unsupervised learning）:无标签建模，数据自我介绍的过程。
        聚类（clustering）:将数据分组。
        降维（dimensionality reduction）:让数据更简洁。
    半监督学习（semi-supervised learning）:数据标签不完整。
二、Scikit-learn
    1.数据表示（data Representation）:
        数据表--->特征矩阵（features matrix）--->目标（标签）数组（target array）
    2.API
    特点：
        统一性：所有对象共同连接一组方法和统一文档。
        内省：所有参数都是公共属性。
        限制对象层级：只有算法可以用类表示，数据集都是标准数据集表示（numpy数组、pandas dataframe、scipy稀疏矩阵）表示，参数名都是标准python字符串
        函数组合：很多任务都可以用一串基本算法实现。
        明智的默认值：当需要用户设置参数时sklearn预先定义了适当的默认值。
    常用步骤:
        （1）从Scikit-learn中导入适当的评估类，选择模型类。
        （2）用合适的数值对模型类进行实例化，配置模型超参数（hyperparameter）。
        （3）整理数据，通过前面介绍的方法获取特征矩阵和目标数组。
        （4）调用模型实例的fit()方法对数据进行拟合。
        （5）对新数据应用模型。
            监督学习：使用predict()方法预测新数据的标签。
            无监督学习：使用transform()或predict()方法转换或推断数据的性质。
三、超参选择和模型验证
    模型选择和超参选择是有效使用各种机器学习工具和技术的最重要阶段。我们需要一种方式来验证选中的模型和参数是否可以很好的拟合数据。
    1.模型验证（model validation）：选择模型和超参后，用训练数据进行学习，对比模型对一直数据的预测值和实际值的差异。
        错误方法：简单存储数据，然后把新数据于存储的一直数据进行对比来预测标签。
        正确方法：
            留出集（holdout set）:sklearn.model_selection.train_test_split()
                缺点：模型失去了一部分训练机会。
            交叉验证（cross validation）：将数据分成若干子集，依次选取每个子集作为验证集。
                1.n轮交叉验证
                from sklearn.model_selection import cross_val_score
                cross_val_score(model,X,y,cv=5)表示5轮交叉验证
                cross_val_predict:cross_val_score 很相像，不过不同于返回的是评测效果，cross_val_predict 返回的是estimator 的分类结果（或回归值）
                2.K折交叉验证:
                    这是将数据集分成K份的官方给定方案，所谓K折就是将数据集通过K次分割，使得所有数据既在训练集出现过，又在测试集出现过，当然，每次分割中不会有重叠。相当于无放回抽样。kf = KFold(n_splits=2)表示
                from sklearn.model_selection import KFold
                
                2.留P法（leavePout）:交叉验证的极端情况，每次留一个样本做验证。与lpo = LeavePOut(p=2)类似
                from sklearn.model_selection import LeaveOneOut
                from sklearn.model_selection import LeavePOut
                scores = cross_val_score(model,X,y,cv=LeaveOneOut(len(x)))
                scores.mean()：均值可以反映模型的准确性。
                3.ShuffleSplit
                    ShuffleSplit 咋一看用法跟LeavePOut 很像，其实两者完全不一样，LeavePOut 是使得数据集经过数次分割后，所有的测试集出现的元素的集合即是完整的数据集，即无放回的抽样，而ShuffleSplit 则是有放回的抽样，只能说经过一个足够大的抽样次数后，保证测试集出现了完成的数据集的倍数。
                4.StratifiedKFold
                    这个就比较好玩了，通过指定分组，对测试集进行无放回抽样。
                5.GroupKFold
                    这个跟StratifiedKFold 比较像，不过测试集是按照一定分组进行打乱的，即先分堆，然后把这些堆打乱，每个堆里的顺序还是固定不变的。
                6.eaveOneGroupOut
                    这个是在GroupKFold 上的基础上混乱度又减小了，按照给定的分组方式将测试集分割下来。
                7.LeavePGroupsOut
                    这个没啥可说的，跟上面那个一样，只是一个是单组，一个是多组
                8.GroupShuffleSplit
                    这个是有放回抽样
                9.TimeSeriesSplit
                    针对时间序列的处理，防止未来数据的使用，分割时是将数据进行从前到后切割（这个说法其实不太恰当，因为切割是延续性的。。）
                

    2.选择最优模型
        问题的答案往往与直觉相悖。
        改善模型能力的高低是决定机器学习实践者是否成功的标志
        1.偏差与方差的均衡：找出偏差（bias）与方差（variance）的均衡点。
            偏差（bias）:模型没有足够的灵活性来适应数据所有特征，就成为欠拟合，也称高偏差。
            方差（variance）:模型过于灵活，适应数据所有特征时也适应了随机误差，成为过拟合，也称高方差。
            R²（coefficient of determination）:判定系数
            R²=1 表示模型数据完全吻合
            R²=0 表示模型不比简单取均值更好。
            R²<0 表示模型性能很差
            对于高偏差模型，模型在验证集的表现与训练集的表现类似。
            对于高方差模型，模型在验证集的表现远远不如在训练集的表现。
            原理：
                total sum of squares  explained sum of squares   residual sum of squares
                R²=1-residual sum of squares/total sum of squares
                R²=1-（真实标签-预测标签）²/（真实标签-真实标签均值）²
                R²=1-偏差/方差
                偏差/方差  服从F分布
                正态总体平法和服从X方分布
            验证曲线：横轴：模型复杂度 纵轴：模型得分  训练的分和测试得分曲线
                特征：
                1.训练得分肯定高于验证得分。
                2.使用低复杂度（高偏差）的模型时，训练数据往往欠拟合，既模型对训练数据和测试数据都缺乏预测能力。
                3.使用高复杂度（高方差）模型时，训练数据往往过拟合，说明模型对训练数据预测能力强，对新数据预测能力差。
                4.使用复杂度适中的模型时，验证曲线得分最高。说明在该复杂度条件下偏差和方差达到均衡点。
            Scikit-learn验证曲线：
                多项式回归模型：
                    1次：y=ax+b
                    2次：y=ax**2+bx+c
                from sklearn.preprocessing import PolynomialFeatures 导入有多现实特征的处理器
                from sklearn.linear_model import LinearRegression 导入简单线性回归模型
                from sklearn.pipeline import make_pipeline 管道命令
                影响模型效果的两个因素：模型复杂度，训练数据集规模 
            学习曲线（learning curve）:训练得分，验证得分 学习曲线的特征： 
                1.特定复杂度的模型对较小的数据集容易过拟合，既此时训练的分高，验证得分低。 
                2.特定复杂度的模型对较大的数据集容易欠拟合，既随着数据集的增大，训练的分会不断降低，而验证的分会不断提高。 
                3.模型的验证集的得分永远不会高于训练集的得分，即两条曲线一直在靠近,但永远不会较差。 
                学习曲线的最重要特征： 随着训练样本的增加，分数会收敛到特定值。（如果你的数据多到使模型得分收敛，那么增加训练样本无济于事， 改善模型性能的唯一方法就是换模型，通常是换成更复杂的模型）。 
            验证实践: 三维网格中寻找最优值：多相似次数的搜索范围、回归模型是否拟合截距、模型是否需要标准化处理。 
            from sklearn.model_selection import GridSearchCV 
四、特征工程（feature engineering）: 特征工程：（将任意格式数据转换成具有良好特征的向量形式）找到与问题有关的任何信息，把它们转换成特征矩阵的数值。如：表示分类数据的特征、表示文本特征、表示图像特征、高模型复杂的的衍生特征、处理缺失数据。该过程也称向量化。 
    1.分类特征：非数值型数据类型是分类数据。 独热编码:增加额外的列，让0,1出现的对应的列分别表示每个分类值有或无。 
        from sklearn.feature_extraction import DictVectorizer 
        vec = DictVectorizer(sparse=False, dtype=int)
        sparse=False表示增加列，以0,1的出现与否表示每个分类有/无 
        sparse=True 表示用稀疏矩阵表示 vec.fit_transform(data) 
        分类特征工具： 
            sklearn.feature_extracton.DictVectorizer 
            sklearn.preprocessing.OneHotEncoder 
            sklearn.feature_extraction.FeatureHasher 
    2.文本特征: 单词统计是最常见的方法（统计每个单词出现的次数，放入表格中）。 
        from sklearn.feature_extraction.text import CountVectorizer 
        import pandas as pd vec=CountVectorizer() 
        X=vec.fit_transform(sample) 
        pd.DataFrame(X.toarray(),columns=vec.get_feature_names()) 
        这样就得到了没个单词出现次数的DataFrame 但是该方法的缺点是：原始单词统计会让一些常用词（如is）聚集太高权重，在分类算法中这样并不合理，解决该问题可以用TF-IDF（term frequency-inverse document frequency，词频逆文档频率，其大小与一个词的常见成都成反比）,通过单词在文档中出现的频率来衡量其权重。 
    3.图像特征：通过Scikit-Image模块来实现http://scikit-image.org 
    4.衍生特征：输入特征经过数学变换衍生出来的新特征。（基函数的回归（basis function regression）：将线性回归转换成多项式回归时，并不是通过改变模型，而是通过改变输入数据）。不改变模型，而是改变输出来改善模型效果--核方法（kernel method）
    5.缺失值填充：简单（列均值，中位数，众数），复杂（矩阵填充，模型处理）
        sklearn的Imputer可以实现一般的填充（均值、中位数、众数）
        from sklearn.preprocessing import Imputer
        imp=Imputer(strategy='mean')
        X2=imp.fit_transform(X)
    6.特征管道：
        如要实现以下三个操作：均值填充缺失，衍生特征转换为2次方，拟合线性回归模型。则可用管道操作。
        from sklearn.pipeline import make_pipeline
        model=make_pipeline(Imputer(strategy='mean'),PolynomialFeatures(degree=2),LinearRegression())
        合并三步为一步。
    
               
五、朴素贝叶斯分类（navie bayes classifier）：属于简单分类算法，通常适用于维度较高的数据集。
    1.特征：运行速度快，可调参数少。（快速、粗糙的解决方案）
        朴素：对每种标签的生成模型进行简单假设，就能找到每种类型生成模型的近似解，然后就可以使用贝叶斯分裂
        贝叶斯定理（Bayes's theorem）：描述统计量的条件概率公式，其成立的前提是：每种特征两两相互独立。
            P（L|特征）=P（特征|L）P（L）/P（特征）
            一种方法是标签对应后验概率的比值：P（L1|特征）/P（L2|特征）=P（特征|L1）P（L1）/P（特征|L2）P（L2）
            生成模型：计算每个标签的P（特征|Li）
            为每种标签设置生成模型是朴素贝叶斯分裂器训练的主要过程。
    2.高斯朴素贝叶斯（Gaussian naive Bayes）:假设每个标签的数据都服从简单的高斯分布。
        假设数据服从高斯分布且无协方差（no covariance，既相互独立），只要找出每个标签的所有样本点的均值和标准差再定义一个高斯分布，就可以拟合模型了。
        通常高斯朴素贝叶斯的分类边界是一条二次方曲线。
        缺点：由于高斯朴素贝叶斯最终分类结果只依赖于一开始的模型假设，经常看不到非常好的结果。（特征较多的时候，它扔是一种好方法）。
        贝叶斯主义（Bayesian formalism）：优质特征是它天生支持概率分布，我们可以用predict_proba方法计算样本属于摸个标签的概率。
        
        通过每种类型的生成模型，计算出任意数据点的似然估计P（特征|Li），然后根据贝叶斯定理计算出后延概率比值P（L1|特征）/P（L2|特征）=P（特征|L1）P（L1）/P（特征|L2）P（L2），而P（L1）和P（L2）很容易得到，从而确定每个数据点的最大可能标签。该步骤在sklearn.naive_bayes.GaussianNB评估器中实现
        
    3.多项式贝叶斯（multinomial naive Bayes）：假设特征服从简单多项式分布。
        多项式分布可以描述各种类型样本出现次数的概率，故多项式朴素贝叶斯非常适合用于描述出现次数或者次数比例特征。
        多项式分布：n次独立试验中（结果为发生或不发生），则发生k次的概率为C[n,k]p**k（1-p）**（n-k）
        通常用于文本分裂，其特征都是指待分类文本的单词出现次数（频次）
        可以用混淆矩阵来统计测试数据的真实标签与预测标签的结果。
        混淆矩阵：矩阵每一列代表预测值（predict class），每一行代表的是实际的类别（actual class）。这个名字来源于它可以非常容易的表明多个类别是否有混淆（也就是一个class被预测成另一个class）。如0,1交叉点表示多少实际的0被预测为1，1,1交叉点表示多少实际的1被预测为1
        查准率（Precision）和查全率（Recall） 我们将算法预测的结果分成四种情况： 
        1. 正确肯定（True Positive,TP）：预测为真，实际为真 
        2. 正确否定（True Negative,TN）：预测为假，实际为假 
        3. 错误肯定（False Positive,FP）：预测为真，实际为假 
        4. 错误否定（False Negative,FN）：预测为假，实际为真  
        则： 
        查准率=TP/（TP+FP）例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 
        查全率=TP/（TP+FN）例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。
      
      
    4.朴素贝叶斯应用场景：
        因为其对数据有严格的假设，因此它的训练效果通常比复杂的模型差，但其也有自己的优点：
            1.训练和预测的速度非常快。
            2.直接使用概率预测。
            3.容易理解。
            4.可调参数（如果有的话）较少。
        朴素贝叶斯分类器非常适合哦用于以下应用场景：
            1.假设分布函数与数据匹配（实际中很少见）。
            2.各种类型的区分度很高，模型复杂度不重要。
            3.非常高维度的数据模型复杂度不重要。
            最后两条看似不同其实彼此相关：随着数据集维度的增加，任何两点都不太可能逐渐靠近（毕竟他们得在每个维度上都足够接近才行），也就是说在新维度会增加样本数据信息量的假设条件下，高维度数据的簇中心点比低维度数据的簇中心点更加分散，因此，随着数据维度的不断增加，像朴素贝叶斯这样的简单分类器的分类效果会和复杂分类器一样，甚至更好。——只要有足够的数据，简单的模型也可以非常强大。
            
            
六、线性回归
    简单线性回归：一维数据
    基函数：对原始数据进行变换，将线性回归模型转化为非线性回归模型。
    y=a0+a1x+a2x**2+......该模型仍然是线性模型，也就是说稀疏an并不会彼此相乘或者相除。我们其实将一维的x投影到了高位空间，因此通过线性模型即可拟合出x和y的关系。
    高斯基函数：sklearn并没有内置高斯基函数
    虽然线性回归引入基函数会让模型变得更加灵活，但是也很容易造成过拟合。
    正则化：当基函数重叠的时候，通常就表明出现了过拟合：相邻基函数的系数相互抵消，这显然是有问题的，如果对较大的模型参数进行惩罚（penalize）,从而抑制模型剧烈波动，应该就可以解决这个问题了，这个惩罚机制被称为正则化（regularization）。
    岭回归（L2范数正则化）：正则化最常见的形式是岭回归（ridge regression）,又称L2范数正则化，又称吉洪诺夫正则化（Tikhonov regularization）。其处理方法是对模型系数平方和（L2范数）进行惩罚，模型拟合的惩罚项为：P=alpha*sum(系数平方）。alpha是一个自由参数用来控制惩罚力度。参数alpha时控制最终复杂度的关键，如果alpha>>0模型就恢复到标准线性回归结果，如果alpha>>正无穷，那么所有模型响应都会被压制。岭回归的一个重要优点是，它可以非常高效的计算——因此相比原始的线性回归模型，几乎没有消耗更多的计算机资源。
    Lasso正则化（L1范数）：其处理方法是对模型系数绝对值的和（L1范数）进行惩罚，P=alpha*sum(系数绝对值）。由于其几何特性，Lasso正则化倾向于构建稀疏模型，也就是说，它更喜欢将模型稀疏设为0。
    


七、支持向量机（support vector machine, SVM）
    SVM是非常强大的有监督学习算法，既可以用于分类也可以用于回归。
    生成分类法：先对每个类进行了随机分布的假设，然后用生成的模型估计新数据点的标签。
    判别分类法：不为每类数据建模，而是用一条分割线（二维空间中的直线或曲线）或者流行体（多维空间中的曲线、曲面等概念的推广）将各种类型分隔开。
        SVC（support vector classifier）：线性边界
        函数距离:γiˆ=yi(w∗xi+b)  已经远离的点可能通过调整中间线使其能够更加远离
        欧式距离：(Ax+By+C)/sqr(A**2+B**2)  不关心已经确定远离的点，更考虑靠近中间分割线的点
        优点之一：对远离数据点边界的数据点不敏感。
        将SVM模型与核函数组合使用，功能非常强大。将数据投影到高维空间，选择基函数比较困难：
            1.计算基函数在数据集上每个点的变换结果，让SVM算法从所有结果中筛选出最优解，这种基函数变换方式被称为核变换，是基于每对数据点之间的相似度（核函数）计算的。SVM模型自带程序来隐式计算核变换数据的拟合，既不须建立了完全的N维核函数的投影空间。在sklearn中我们可以应用核函数化的SVM模型将线性核转变为RBF（径向基函数）核，设置kernel模型超参即可。在机器学习中，核变换策略经常用于将快速线性方法变换成快速非线性方法，尤其是对于那些可以应用核函数技巧的模型。

    SVM优化：软化边界
        对于有交叉的数据，并没有清晰的分类边界，SVM提供了一些修正因子来软化边界，为了取得更好的拟合结果，它允许一些点位于边界线内。
        边界硬度C：如果C很大，边界就很硬，数据点不能再边界内生存，如果C小，边界线较软，有一些数据点可以穿越边界线。    
    
    总结：
        优点：
            1.模型依赖的支持向量较少，说明它模型精致，消耗内存少。
            2.一旦训练完成，预测速度快。
            3.由于模型只受边界线附近的点影响，因此他们对于高维数据的学习效果非常好。
            4.与核函数方法的配合极具通用性，能适应不同类型的数据。
        缺点：
            1.样本量N的增加，最差的训练时间复杂度会达到O(N**3)。经过高效处理后，也只能达到O(N**2)。大样本学习成本高。
            2.训练效果非常依赖于边界软化参数C的选择是否合理，这需要通过交叉检验自行搜索，当数据集大时，计算量大。
            3.预测结果不能直接进行概率解释，这一点可以通过内部交叉检验进行评估(VC的probability)但是评估过程计算量也非常大。
            
        由于这些条件的限制通常在其他简单、快速、调优难度小的方法不能满足需求时，才选择支持向量机，但是如果你对计算资源足以支撑SVM对数据集的训练和交叉检验，那么一定会获得很好的效果。
        


八、决策树与随机森林——无参数、集成方法 
    通过集成多个比较简单的评估模型形成积累效果，学习效果往往能超过多个简单评估模型形成的总和。既若干评估器的多数投票(ajority vote)的最终效果往往优于单个评估器投票的结果。
    1.随机森林是建立在决策树基础上的集成学习器，决策树直观的对事物进行分类和打标签，你只需问一系列问题就可以进行分类。
        二叉决策树：
            二叉树分支方法可以非常有效的进行分类一颗结构合理的决策树每个问题都可以将种类可能性减半，即使是对大量种类进行决策时，也可以很快缩小选择范围。问题：决策树分类边界是与特征轴平行的形式分割数据，既决策树每个节点都根据一个特征的法制将数据分成两组。
        决策树过拟合：
            决策树过拟合是决策树的一般属性——决策树非常容易陷得很深，因此往往会拟合局部数据，而没有对整个数据分布的大局观，这种过拟合就是模型训练的是数据的不同子集
            
            RandomForestRegressor可以拟合震荡曲线，适合处理多周期数据，不需要配置多周期模型。
            RandomForestClassifier用多棵决策树拟合曲线达到更好的效果
            

    2.评估器集成法：随机森林 BaggingClassifier
        装袋算法：通过组合多个过拟合评估器来降低过拟合程度的想法其实是一种集成学习方法。
        随机森林：装袋算法中使用平行评估器对数据进行有放回抽取集成（大杂烩），每个评估器都会对数据过拟合，通过求均值可以获得更好的分类结果。
    3.总结：
        随机森林优势：
            1.决策原理简单，故而它的训练和预测速度都非常快。多任务可以直接并行（多进程）计算（n_job参数），因为每棵树都是完全独立的。
            2.多棵树可以进行概率分类，多个评估器之间的多数投票可以给出概率的估计值（使用sklearn的predict_proba()方法）。
            3.无参数模型很灵活
            
            
九、主成分分析
    主成分分析（principal component analysis, PCA）:是一种基础降维算法，尤其适用于可视化、噪音过滤、特征抽取、和特征工程等领域。
    回归分析中我们希望根据x预测y,而无监督学习我们希望探索x和y之间的相关性。
    pca.components_:成分
    pca.explained_variance_：可解释差异
    每个数据点在主轴上的投影就是数据的主成分。
    仿射变换：从数据的坐标轴变换到主轴的变换，包含平移（translation）、旋转（rotation）和均匀缩放（uniform scaling）三个步骤。
    PCA降维：pca.transform(X),降维后虽然50%的数据维度被缩减，但是数据的总体关系仍然被大致保留下来。
    pca.explained_variance_ratio_：累计方差贡献率
    特征数与累计方差贡献率曲线可以用来确定选择的PCA模型的特征数。
    PCA噪声过滤：任何成分的方差都远大于噪音的方差，故而如果使用主成分的最大子集重构数据，就可以实现选择性保留信号并且丢弃噪音。
    总结：
        本节主要讨论用主成分分析进行，降维、高维数据可视化、噪音过滤、和高维数据特征选择。
        弱点：经常受到数据集异常点的影响，RandomizedPCA和SparsePCA这两个算法丢弃了原始成分很糟糕的点。RandomizedPCA快速地近似计算出一个维度非常高的数据的前几个主成分，二SparsePCA引入了一个正则项来保证成分的稀疏性。
        对非线性关系的数据集的处理效果并不好。


十、流形学习（manifold learning）
    可以弥补PCA对非线性数据处理效果不好的缺陷，是一种无监督评估器，它将一个低维度流形嵌入到一个高维度空间来描述数据集。
    流形学习可以理解为把嵌入三维空间中的二维流形（三维空间一个剖面）
    几种流形方法技巧：多维标度法（multidimensional scaling, MDS）,局部嵌入法（locally linear embedding, LLE）,保距映射法（isometric mapping, Isomap）
    from sklearn.metrics import pairwise_distances可以计算原始数据的关系矩阵。
    多维标度法（multidimensional scaling, MDS）：依靠数据点间关系矩阵，就可以还原出一种可行的二维坐标。
        sklearn.manifold.MDS可以还原距离矩阵。dissimilarity=precomputed
        流形学习评估器希望达成目标：给定一个高位嵌入数据，寻找数据的一个低维表示，并保留数据间的特定关系。
        MDS是线性嵌入模型，它包括将数据旋转、平移和缩放到一个高维度空间的操作。
    但是当嵌入为非线性时，既超越简单的操作集合时，MDS算法会失败，因为使用简单的MDS算法来处理这个数据，就无法展示数据非线性嵌入的特征，从而导致我们丢失了这个嵌入式流形的内部基本关系特征。
    非线性流形：局部线性嵌入（locally linear embedding, LLE）
        局部线性嵌入可以从非线性嵌入数据中恢复潜在数据特征
        该方法不保留所有数据点之间的距离，只保留临近点之间的距离。
        它通过对成本函数的全局优化来反映这个逻辑。
        from sklearn.manifold import LocallyLinearEmbedding
        n_neighbors=100表示取得临近点个数。
    流形学习在实际应用中的要求非常严格，因此除了对高维度数据进行简单的定性可视化之外，流形学习很少被正式使用。
    缺点：
        1.流形学习中没有好的框架处理缺失值，相比之下PCA中有一个用于处理缺失值的迭代方法。
        2.在流形学习中，数据噪音的出现将造成流形短路，并严重影响嵌入结果，相比下PCA可以自然地从最重要的成分中滤除噪音。
        3.流形嵌入的结果通常高度依赖所选取的邻节点个数，并且通常没有确定的定量方式来选择最优邻节点数。相比下PCA并无此问题。
        4.流形学习中全局最优的诶输出维度数很难确定，相比下PCA可以基于解释方差来确定输出的维度数。
        5.流形学习中嵌入维度的含义并不总是很清楚，而在PCA算法中，主成分有明确含义。
        6.流形学习中，流形算法复杂度为O[N**2]或O[N**3]，而PCA可以选择随机方法，通常速度更快。
    虽然流形算法有缺点，但其有一明显优点（保留数据非线性关系的能力），所以通常做法是：先用PCA探索数据的线性特征，再用流形方法探索非线性特征。
    其他几种流形算法：
        1.LLE在sklearn.manifold.LocallyLinearEmbedding中实现。它对于简单问题，它对于S曲线，局部线性嵌入（LLE）及其变体（特别是modified LLE）的学习效果很好。
        2.Isomap在sklearn.manifold.Isomap中实现，虽然LLE通常对现实世界的高位数据源的学习效果差，但是Isomap算法往往会获得较好的嵌入效果。
        3.t-分布邻域嵌入法（t-distributed stochastic neighbor embedding, t-SNE）,在sklearn.manifold.TSNE中实现，将他用于高度聚类的数据效果比较好，但是该方法比其他方法学习速度慢。


十一：k-means聚类
    聚类算法：直接从数据的内在性质总学习最优的划分结果或者确定离散标签类型。
    k-means算法在不带标签的多维数据集中寻找确定数量的簇。最优聚类结果要符合以下假设：
        1.簇中心点（cluster center）是属于该簇的所有数据点坐标的算术平均值。
        2.一个簇的每个点到簇中心点的距离比到其他簇中心点的的距离短。
    k-means采用期望最大化算法取代穷举搜索。
        穷举法：穷举法的基本思想是根据题目的部分条件确定答案的大致范围，并在此范围内对所有可能的情况逐一验证，直到全部情况验证完毕。若某个情况验证符合题目的全部条件，则为本问题的一个解；若全部情况验证后都不符合题目的全部条件，则本题无解。穷举法也称为枚举法。
        最大期望法（expectation-maximization，E-M）：
            1.猜测一些簇中心点。
            2.重复直至收敛。
                a.期望步骤（E-step）：将点分配至离其最近的簇中心点。
                b.最大化步骤（M-step）：将簇中心点设置为所有点坐标的平均值。
                E-step不断更新每个点是属于哪一个簇的期望值，M-step计算关于簇中心点的拟合函数值最大化对应坐标（argmax函数）——k-means中，通过简单求每个簇中所有数据点坐标的平均值得到了簇中心坐标。在典型环境下，每次重复E-step和M-step都会的到更好的聚类效果。
            使用最大期望法可能不会达到全局最优结果，首先虽然E-M算法可以在每一步中改进结果，但是它并不保证可以获得全局最优解决方案。k-means还有一个显著问题：你必须告诉该算法数量，因为它无法从数据中自动学习到簇的数量。
            轮廓分析法可以直观反映簇数量的取值是否合适。
            Gaussian mixture models可以很好的对每个簇的聚类效果进行度量。
            DBSCAN、均值漂移、临近传播等方法也可以用来选择簇数量
    最邻近图（the graph of the nearest neighbors）：
    k-means聚类的边界总是线性的，这就意味着当边界很复杂时，算法会失效，这时则可以通过核变换将数据投影到更高维的空间，投影后的数据使线性分离称为可能，这种算法在sklearn.SpectralClustering中实现，它使用最邻近图（the graph of the nearest neighbors）来计算数据的高维表示，然后用k-means算法分配标签。
    批处理batch-based）k-means：
    当数据量较大时k-means会很慢，由于k-means的每次迭代都必须获取数据集中所有点，因此随着数据量增加，算法会变得很慢。因此可以每一步使用数据集的一个子集来更新簇中心点。这就是批处理（batch-based）k-means算法的核心思想。sklearn.cluster.MiniBatchKMeans。
    t-分布邻域嵌入算法：sklearn.manifold.TSNE，是一种非线性嵌入算法，擅长保留簇中的数据点。

十二、高斯混合模型（Gaussian mixture model,GMM）
    from sklearn.mixture import GMM
    k-mean的非概率性，和它根据到簇中心点的距离来指派簇的特点将导致性能地下。本节介绍混合模型该模型是k-means思想的一个扩展。
    k-means要求这些簇的模型必须是圆形：其内置的方法来实现椭圆形的簇。因此如果对同样的数据进行一些转换（数据的簇不是圆形），簇的分配就会变得混乱。
    k-means两个缺点：
        1.类形状缺少灵活性、2.缺少簇分配的概率，这两个缺点使得它对许多数据集（特别是低维数据集）的拟合效果不尽如人意。
    GMM有一个隐含的概率模型，它可以找到簇分配的概率结果--predict_proba,结果为任意点属于某个簇的概率。这个算法有时并不是全局最优解，因此在实际使用中需使用多个随机初始解。
    GMM 中covariance_type选项的设置是不同的，这个超参控制了每个簇的形状自由度，默认值是‘diag’意思是簇在每个维度的尺寸都可以单独设置，椭圆边界的主轴与坐标轴平行。
    covariance_type=‘spherical’表示该模型通过约束簇的形状让所有维度相等。
    covariance_type='full'表示该模型允许每个簇在任意方向上用椭圆建模。
    GMM通常被归类为聚类算法，但它本质上是一个密度估计算法，既GMM拟合的结果并不是一个聚类模型，而是描述数据分布的生成概率模型。
    赤池信息量准则（Akaike information criterion, AIC）、贝叶斯信息准则（Bayesian information criterion, BIC）可以用来调整模型的似然估计来纠正过拟合。
    成分数量的选择度量是GMM作为一个密度评估器的性能，而不是作为一个聚类算法的性能。建议你还是把GMM当成一个密度评估器，仅在简单数据集中才将它作为聚类算法使用。


十三、核密度估计
    密度估计是一种通过D维数据集生成D维概率分布估计的算法。GMM算法用不同高斯分布的甲醛汇总来表示概率分布估计，核密度估计（kernel density estimation, KDE）算法将高斯混合理念扩展到了逻辑极限（logical extreme）。它通过对每个点生成高斯分布的混合成分，获得本质上无参数的密度评估器。
    核密度估计的自由参数是核类型（kernel）参数，它可以指定每个点核密度分布的形状。而核带宽（kernel bandwidth）参数控制每个点的核的大小。在实际应用中，有很多核可用于核密度估计。KDE（kernel density estimation）支持六核。
    sklearn.neighbors.KernelDensity,atol(绝对容错），rtol(相对容错)参数来平衡计算时间与准确性，可以用交叉验证工具来确定自有参数核带宽（kernel bandwidth）。
    核带宽不仅对找到了合适的密度估计非常重要，也是在密度估计中控制偏差-方差平衡的关键：带宽过窄将导致估计呈现高方差（既过拟合），而且每个点的出现或缺失都会引起很大的不同，带宽过宽将导致估计呈现高偏差（欠拟合），而且带宽较大还会破坏数据结构。
    用KDE生成非猎奇算法：
        1.通过标签分割训练数据
        2.为每个集合拟合一个KDE来获得数据的生成模型，这样就可以用任意x观察值和y标签计算出似然估计值P（x|y）。
        3.根据训练集中每一类的样本数量，计算每一类的先验概率P（y）。
        4.对于一个未知点x,每一类的后延概率P（x|y）P（y）,而后验概率最大的类就是分配给该点的标签。
    在sklearn中每个评估器都是一个类，这个类可以很方便的集成字BaseEstimator(其中包含各种标准功能)，也支持试单搞得混合类（mixin, 多重集成方法）。所有拟合结果都存储在带后下划线的变量中如：self.logpriors_。
    方向梯度直方图（Histogram of Oriented Gradients, HOG）可以将图像像素转换成向量形式，与图像具体内容有关，与图像合成因素无关。
    HOG方法包含以下步骤。
        1.图像标准化，消除照度对图像的影响。
        2.用与水平和垂直方向的亮度梯度相关的两个过滤器处理图像，捕捉图像的变，角和纹理信息。
        3.将图像切割成预定义大小的图块，然后计算每个图块内荼毒方向频次直方图。
        4.对比每个图块与相邻图块的频次直方图，并做标准化处理，进一步消除照度对图像的影响。
        5.获得描述每个图块信息的一维特征向量。
        sklearn.Image内置了快速HOG提取器。
    难负样本挖掘（hard negative mining）：给分类器看一些没见过的新图像，找出所有分类器识别错误的假正图像，然后将这些图像增加到训练集中，再中心训练分类器。
    
    
十五、机器学习参考资料
    1.sklearn 官网http://scikit-learn.org
    2.Scipy、PyCon和PyData教程
    3.《机器学习入门》Andreas C. Mueller 和 Sarah Guido
    4.《Python 机器学习》Sebastian Raschka
    5.通用机器学习资源
        Machine Learning（https://www.coursera.org/learn/machine-learning）
　　
        Pattern Recognition and Machine Learning（http://www.springer.com/us/book/9780387310732）
　　
        Machine Learning: A Probabilistic Perspective（https://mitpress.mit.edu/books/machine-learning-0）
    
    
    