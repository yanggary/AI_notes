{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、机器学习：借助数学模型理解数据。学习过程即为调整参数让模型拟合得到的统计数据。\n",
    "    分类：监督学习（supervised learning），无监督学习(unsupervised learning)，半监督学习(semi-supervised learning)\n",
    "    监督学习（supervised learning）:对数据的若干特征和标签（类型）之间的关联性进行建模的过程。\n",
    "        分类（classification）：标签（分类）是离散值，预测分类即定性分析。\n",
    "        回归（regression）：标签是连续值，预测值即定量分析。\n",
    "    无监督学习（unsupervised learning）:无标签建模，数据自我介绍的过程。\n",
    "        聚类（clustering）:将数据分组。\n",
    "        降维（dimensionality reduction）:让数据更简洁。\n",
    "    半监督学习（semi-supervised learning）:数据标签不完整。\n",
    "二、Scikit-learn\n",
    "    1.数据表示（data Representation）:\n",
    "        数据表--->特征矩阵（features matrix）--->目标（标签）数组（target array）\n",
    "    2.API\n",
    "    特点：\n",
    "        统一性：所有对象共同连接一组方法和统一文档。\n",
    "        内省：所有参数都是公共属性。\n",
    "        限制对象层级：只有算法可以用类表示，数据集都是标准数据集表示（numpy数组、pandas dataframe、scipy稀疏矩阵）表示，参数名都是标准python字符串\n",
    "        函数组合：很多任务都可以用一串基本算法实现。\n",
    "        明智的默认值：当需要用户设置参数时sklearn预先定义了适当的默认值。\n",
    "    常用步骤:\n",
    "        （1）从Scikit-learn中导入适当的评估类，选择模型类。\n",
    "        （2）用合适的数值对模型类进行实例化，配置模型超参数（hyperparameter）。\n",
    "        （3）整理数据，通过前面介绍的方法获取特征矩阵和目标数组。\n",
    "        （4）调用模型实例的fit()方法对数据进行拟合。\n",
    "        （5）对新数据应用模型。\n",
    "            监督学习：使用predict()方法预测新数据的标签。\n",
    "            无监督学习：使用transform()或predict()方法转换或推断数据的性质。\n",
    "三、超参选择和模型验证\n",
    "    模型选择和超参选择是有效使用各种机器学习工具和技术的最重要阶段。我们需要一种方式来验证选中的模型和参数是否可以很好的拟合数据。\n",
    "    1.模型验证（model validation）：选择模型和超参后，用训练数据进行学习，对比模型对一直数据的预测值和实际值的差异。\n",
    "        错误方法：简单存储数据，然后把新数据于存储的一直数据进行对比来预测标签。\n",
    "        正确方法：\n",
    "            留出集（holdout set）:sklearn.model_selection.train_test_split()\n",
    "                缺点：模型失去了一部分训练机会。\n",
    "            交叉验证（cross validation）：将数据分成若干子集，依次选取每个子集作为验证集。\n",
    "                1.n轮交叉验证\n",
    "                from sklearn.model_selection import cross_val_score\n",
    "                cross_val_score(model,X,y,cv=5)表示5轮交叉验证\n",
    "                2.留一法（leave one out）:交叉验证的极端情况，每次留一个样本做验证。\n",
    "                from sklearn.model_selection import LeaveOneOut\n",
    "                scores = cross_val_score(model,X,y,cv=LeaveOneOut(len(x)))\n",
    "                scores.mean()：均值可以反映模型的准确性。\n",
    "    2.选择最优模型\n",
    "        问题的答案往往与直觉相悖。\n",
    "        改善模型能力的高低是决定机器学习实践者是否成功的标志\n",
    "        1.偏差与方差的均衡：找出偏差（bias）与方差（variance）的均衡点。\n",
    "            偏差（bias）:模型没有足够的灵活性来适应数据所有特征，就成为欠拟合，也称高偏差。\n",
    "            方差（variance）:模型过于灵活，适应数据所有特征时也适应了随机误差，成为过拟合，也称高方差。\n",
    "            R²（coefficient of determination）:判定系数\n",
    "            R²=1 表示模型数据完全吻合\n",
    "            R²=0 表示模型不比简单取均值更好。\n",
    "            R²<0 表示模型性能很差\n",
    "            对于高偏差模型，模型在验证集的表现与训练集的表现类似。\n",
    "            对于高方差模型，模型在验证集的表现远远不如在训练集的表现。\n",
    "            原理：\n",
    "                total sum of squares  explained sum of squares   residual sum of squares\n",
    "                R²=1-residual sum of squares/total sum of squares\n",
    "                R²=1-（真实标签-预测标签）²/（真实标签-真实标签均值）²\n",
    "                R²=1-偏差/方差\n",
    "                偏差/方差  服从F分布\n",
    "                正态总体平法和服从X方分布\n",
    "            验证曲线：横轴：模型复杂度 纵轴：模型得分  训练的分和测试得分曲线\n",
    "                特征：\n",
    "                1.训练得分肯定高于验证得分。\n",
    "                2.使用低复杂度（高偏差）的模型时，训练数据往往欠拟合，既模型对训练数据和测试数据都缺乏预测能力。\n",
    "                3.使用高复杂度（高方差）模型时，训练数据往往过拟合，说明模型对训练数据预测能力强，对新数据预测能力差。\n",
    "                4.使用复杂度适中的模型时，验证曲线得分最高。说明在该复杂度条件下偏差和方差达到均衡点。\n",
    "                Scikit-learn验证曲线：\n",
    "                    多项式回归模型：\n",
    "                        1次：y=ax+b\n",
    "                        2次：y=ax^2+bx+c \n",
    "                    from sklearn.preprocessing import PolynomialFeatures 导入有多现实特征的处理器\n",
    "                    from sklearn.linear_model import LinearRegression 导入简单线性回归模型\n",
    "                    from sklearn.pipeline import make_pipeline 管道命令\n",
    "                    影响模型效果的两个因素：模型复杂度，训练数据集规模\n",
    "            学习曲线（learning curve）:训练得分，验证得分\n",
    "                学习曲线的特征：\n",
    "                1.特定复杂度的模型对较小的数据集容易过拟合，既此时训练的分高，验证得分低。\n",
    "                2.特定复杂度的模型对较大的数据集容易欠拟合，既随着数据集的增大，训练的分会不断降低，而验证的分会不断提高。\n",
    "                3.模型的验证集的得分永远不会高于训练集的得分，即两条曲线一直在靠近,但永远不会较差。\n",
    "                学习曲线的最重要特征：\n",
    "                随着训练样本的增加，分数会收敛到特定值。（如果你的数据多到使模型得分收敛，那么增加训练样本无济于事，\n",
    "                改善模型性能的唯一方法就是换模型，通常是换成更复杂的模型）。\n",
    "            验证实践:\n",
    "                三维网格中寻找最优值：多相似次数的搜索范围、回归模型是否拟合截距、模型是否需要标准化处理。\n",
    "                from sklearn.model_selection import GridSearchCV\n",
    "四、特征工程（feature engineering）:\n",
    "    特征工程：（将任意格式数据转换成具有良好特征的向量形式）找到与问题有关的任何信息，把它们转换成特征矩阵的数值。如：表示分类数据的特征、表示文本特征、表示图像特征、高模型复杂的的衍生特征、处理缺失数据。该过程也称向量化。\n",
    "    1.分类特征：非数值型数据类型是分类数据。\n",
    "        独热编码:增加额外的列，让0,1出现的对应的列分别表示每个分类值有或无。\n",
    "        from sklearn.feature_extraction import DictVectorizer\n",
    "        vec = DictVectorizer(sparse=False, dtype=int)\n",
    "        sparse=False 表示增加列，以0,1的出现与否表示每个分类有/无\n",
    "        sparse=True  表示用稀疏矩阵表示\n",
    "        vec.fit_transform(data)\n",
    "            分类特征工具：\n",
    "                sklearn.feature_extracton.DictVectorizer\n",
    "                sklearn.preprocessing.OneHotEncoder\n",
    "                sklearn.feature_extraction.FeatureHasher\n",
    "        文本特征:\n",
    "            单词统计是最常见的方法（统计每个单词出现的次数，放入表格中）。\n",
    "            from sklearn.feature_extraction.text import CountVectorizer\n",
    "            import pandas as pd\n",
    "            vec=CountVectorizer()\n",
    "            X=vec.fit_transform(sample)\n",
    "            pd.DataFrame(X.toarray(),columns=vec.get_feature_names())\n",
    "            这样就得到了没个单词出现次数的DataFrame\n",
    "            但是该方法的缺点是：原始单词统计会让一些常用词（如is）聚集太高权重，在分类算法中这样并不合理，解决该问题可以用TF-IDF（term frequency-inverse document frequency，词频逆文档频率，其大小与一个词的常见成都成反比）,通过单词在文档中出现的频率来衡量其权重。\n",
    "        图像特征：通过Scikit-Image模块来实现http://scikit-image.org\n",
    "        衍生特征：输入特征经过数学变换衍生出来的新特征。（基函数的回归（basis function regression）：将线性回归转换成多项式回归时，并不是通过改变模型，而是通过改变输入数据）。不改变模型，而是改变输出来改善模型效果--核方法（kernel method）\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
